{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----\n",
    "# FIG 8 in RECCAP SO paper -> compute trends for different time periods\n",
    "# -----\n",
    "# trends in simB are loaded from netcdf file produced by: PAPER_RECCAPv2_SO_Suppl_time_series_trends_simB.ipynb\n",
    "#\n",
    "# Plot time series of CO2 fluxes \n",
    "# drift-corrected\n",
    "# \n",
    "# Contact: cara.nissen@awi.de or cara.nissen@colorado.edu\n",
    "#\n",
    "# version: August 2023\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1528226/1677564082.py:23: MatplotlibDeprecationWarning: \n",
      "The mpl_toolkits.axes_grid module was deprecated in Matplotlib 2.1 and will be removed two minor releases later. Use mpl_toolkits.axes_grid1 and mpl_toolkits.axisartist, which provide the same functionality instead.\n",
      "  from mpl_toolkits.axes_grid.inset_locator import inset_axes\n"
     ]
    }
   ],
   "source": [
    "### modules\n",
    "import os\n",
    "import numpy as np\n",
    "import seawater as sw\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm \n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "from netCDF4 import Dataset\n",
    "from datetime import date, timedelta\n",
    "import copy\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.ticker import (LongitudeFormatter, LatitudeFormatter,\n",
    "                                LatitudeLocator)\n",
    "from cartopy.util import add_cyclic_point\n",
    "import cartopy.feature as cfeature\n",
    "from annualmean import annualmean\n",
    "from datetime import date, timedelta\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from mpl_toolkits.axes_grid.inset_locator import inset_axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#-----\n",
    "# SETTINGS\n",
    "# define paths to all data\n",
    "# define where to save plots (if any)\n",
    "# define years to average over\n",
    "#-----\n",
    "\n",
    "# data sets\n",
    "path1 = '/pscratch/sd/c/cnissen/RECCAPv2/'\n",
    "path_models = path1+'reccap_submissions/download_20220124/Models/2D_CO2/' \n",
    "path_data   = path1+'reccap_submissions/download_20220124/Surface_CO2/'\n",
    "path_atminv = path1+'reccap_submissions/download_20220124/Atmospheric_inversions/'\n",
    "path_soccom = path1+'reccap_submissions/download_20220124/Surface_CO2/'\n",
    "path_trend= path1+'reccap_submissions/download_20220124/Models/Linear_trends/' \n",
    "\n",
    "# river flux adjustment\n",
    "path_river  = path1+'masks_reccap/river_flux_adjustment/'\n",
    "\n",
    "# path to RECCAP SO mask\n",
    "path_mask = path1+'masks_reccap/'\n",
    "\n",
    "#----\n",
    "# specify years to average over\n",
    "#----\n",
    "# NOTE: the script is written to plot the time series from 1985-2018\n",
    "#    the years are defined here for plot titles and filenames \n",
    "year1,year2 = 1985,2018 \n",
    "eval_time   = np.arange(year1,year2+1) \n",
    "\n",
    "#----\n",
    "# define simulation\n",
    "#----\n",
    "sim = 'A'\n",
    "\n",
    "#----\n",
    "# define where to save plots/txt files\n",
    "#----\n",
    "save_to_dir ='/global/cfs/cdirs/m4003/cnissen/Plots/RECCAPv2_SO_Paper/Fig_8/'\n",
    "if not os.path.exists(save_to_dir):\n",
    "    print ('Created '+save_to_dir)\n",
    "    os.makedirs(save_to_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of MPI: 7\n",
      "All inidces except MPI: [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13]\n",
      "Models: 14 14\n",
      "Data products: 9 9\n",
      "Data Watson2020: 1 1\n",
      "Data assimilating models: 2 2\n",
      "Data OCIM: 2 2\n",
      "Data ATM inversion: 6 1\n"
     ]
    }
   ],
   "source": [
    "#-----\n",
    "# define data sets to consider\n",
    "#-----\n",
    "# NOTE: there is 6 different atm. inversions in the provided file, only load the ones that start in 1990 \n",
    "# NOTE: AOML has to be the first in the list! there is a few places where the exclusion of AOML is \n",
    "# hard-coded as \"[1:]\"\n",
    "\n",
    "# variable name of interest\n",
    "var = '2D_CO2'\n",
    "\n",
    "models     = ('CCSM-WHOI','CESM-ETHZ','CNRM-ESM2-1','EC-Earth3','FESOM_REcoM_HR','FESOM_REcoM_LR',\\\n",
    "             'MOM6-Princeton','MPIOM-HAMOCC','MRI-ESM2-1','NorESM-OC1.2',\\\n",
    "             'ORCA025-GEOMAR','ORCA1-LIM3-PISCES','PlankTOM12','ROMS-SouthernOcean-ETHZ') \n",
    "models2     = ('CCSM-WHOI','CESM-ETHZ','CNRM-ESM2-1','EC-Earth3','FESOM_REcoM_HR','FESOM_REcoM_LR',\\\n",
    "             'MOM6-Princeton','MPIOM-HAMOCC','MRI-ESM2-1','NorESM-OC1.2',\\\n",
    "             'ORCA025-GEOMAR','ORCA1-LIM3-PISCES','PlankTOM12','ROMS-SO-ETHZ') \n",
    "ind_mpi = models.index(\"MPIOM-HAMOCC\")\n",
    "ind_not_mpi = [i for i, s in enumerate(models) if 'MPIOM-HAMOCC' not in s]\n",
    "print ('Index of MPI:',ind_mpi)\n",
    "print ('All inidces except MPI:',ind_not_mpi)\n",
    "data_assim  = ('BSOSE','ECCO-Darwin')\n",
    "data_ocim   = ('OCIM-v2014-CTL','OCIM-v2021')\n",
    "#data_atminv = ['Atm_inv1','Atm_inv2','Atm_inv4']  #'Atm_inv3','Atm_inv5','Atm_inv6' -> start later than 1990\n",
    "data_atminv = ['Atm_inv1','Atm_inv2','Atm_inv3','Atm_inv4','Atm_inv5','Atm_inv6'] # for 2015-2018, consider all 6\n",
    "data_prod   = ('AOML_EXTRAT','CMEMS-LSCE-FFNN','CSIRML6','JenaMLS','JMAMLR',\\\n",
    "             'LDEO-HPD','NIES-ML3','OceanSODAETHZ','SOMFFN') \n",
    "data_watson = ['UOEX_Wat20']\n",
    "soccom      = ('SOCCOM_Jena','SOCCOM_SOMFFN')\n",
    "\n",
    "versionID_models     = ('20211125','v20211122','v20211208','v20220323','v20211119','v20211119',\\\n",
    "                       'v20220125','v20220110','v20220502','v20211125',\\\n",
    "                       'v20210804','v20211215','v20220404','v20220630') \n",
    "versionID_data_assim  = ('I134','v20210712')\n",
    "versionID_data_ocim   = ('v20210607','v20210511')\n",
    "versionID_data_atminv = ['v20211008']\n",
    "versionID_data_prod   = ('v20211130','v20210709','v20211117','v20211126','v20211208',\\\n",
    "                       'v20211210','v20220222','v20211207','v20211121')\n",
    "versionID_data_watson = ['v20211204']\n",
    "\n",
    "# list of filenames for data products\n",
    "filename_data_prod = ('fgco2_AOML_EXTRAT_1997-2020_v20211018.nc4',\\\n",
    "                      'fgco2_CMEMS-LSCE-FFNN_1985-2018_v20210709.nc',\\\n",
    "                      'fgco2_CSIRML6_1985-2018_v20211117.nc',\\\n",
    "                      'fgco2_JenaMLS_1985-2018_v20211126.nc',\\\n",
    "                      'fgco2_JMAMLR_1985-2019_v20211208.nc',\\\n",
    "                      'fgco2_LDEO_HPD_1985-2018_v20211210.nc',\\\n",
    "                      'fgco2_NIES-ML3_1980-2020_v20220222.nc',\\\n",
    "                      'fgco2_OceanSODAETHZ_1985-2018_v20211207.nc',\\\n",
    "                      'fgco2_MPI_SOMFFN_1982-2019_v20211121.nc')\n",
    "filename_data_watson = ['fgco2_UOEX_Wat20_1985-2019_v20211204.nc']\n",
    "\n",
    "subregions = ('STSS-Atl','STSS-Ind','STSS-Pac',\\\n",
    "              'SPSS-Atl','SPSS-Ind','SPSS-Pac',\\\n",
    "              'ICE-Atl','ICE-Ind','ICE-Pac','STSS','SPSS','ICE','all')   \n",
    "\n",
    "print ('Models:',len(models),len(versionID_models))\n",
    "print ('Data products:',len(data_prod),len(filename_data_prod))\n",
    "print ('Data Watson2020:',len(data_watson),len(versionID_data_watson))\n",
    "print ('Data assimilating models:',len(data_assim),len(versionID_data_assim))\n",
    "print ('Data OCIM:',len(data_ocim),len(versionID_data_assim))\n",
    "print ('Data ATM inversion:',len(data_atminv),len(versionID_data_atminv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----\n",
    "# FUNCTIONS\n",
    "#----\n",
    "\n",
    "# transform longitude (e.g., go from -179.5:179.5 to 0.5:359.5)\n",
    "def transform_lon_coord(data):\n",
    "    # change lon coordinate in 2D array from 0-360 to -180:180\n",
    "    # for 2D arrays: assume lon coordinate to be the 2nd dimension  \n",
    "    # for 3D arrays: assume lon coordinate to be the 3rd dimension  \n",
    "    if len(data.shape)==2:\n",
    "        data_transformed          = np.empty_like(data)\n",
    "        data_transformed[:,0:180] = data[:,180:]\n",
    "        data_transformed[:,180:]  = data[:,0:180] \n",
    "    elif len(data.shape)==3:\n",
    "        data_transformed          = np.empty_like(data)\n",
    "        try:\n",
    "            data_transformed[:,:,0:180] = data[:,:,180:]\n",
    "            data_transformed[:,:,180:]  = data[:,:,0:180]\n",
    "        except:\n",
    "            data_transformed[:,0:180,:] = data[:,180:,:]\n",
    "            data_transformed[:,180:,:]  = data[:,0:180,:] \n",
    "    elif len(data.shape)==4:\n",
    "        data_transformed          = np.empty_like(data)\n",
    "        data_transformed[:,0:180,:,:] = data[:,180:,:,:]\n",
    "        data_transformed[:,180:,:,:]  = data[:,0:180,:,:] \n",
    "    elif len(data.shape)==1:\n",
    "        data_transformed          = np.empty_like(data)\n",
    "        data_transformed[0:180] = data[180:]\n",
    "        data_transformed[180:]  = data[0:180]\n",
    "    return data_transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-179.5 179.5\n",
      "0.0 8.0\n"
     ]
    }
   ],
   "source": [
    "#------\n",
    "# load RECCAP mask for SO\n",
    "# make sure region mask matches the submitted products in terms of longitude!\n",
    "#------\n",
    "\n",
    "# SO RECCAP regions\n",
    "#reccap_mask_SO   = Dataset(path_mask+'RECCAP2_region_masks_all_v20221025.nc')\n",
    "#regions          = reccap_mask_SO_2.variables['southern'][:,:] #\"1.SO STSS, 2.SO SPSS, 3.SO ICE\"\n",
    "#lon_regions      = reccap_mask_SO_2.variables['lon'][:]\n",
    "#print(np.min(lon_regions),np.max(lon_regions)) # lon should be from 0-360 (if it is from -180:180, use transform_lon_coord)\n",
    "#print(np.min(regions_2),np.max(regions_2))\n",
    "\n",
    "# SO RECCAP regions\n",
    "reccap_mask_SO = Dataset(path_mask+'reccap_regions_SOsubs.nc')\n",
    "regions        = reccap_mask_SO.variables['SO_basins_biomes'][:,:]\n",
    "lon_regions    = reccap_mask_SO.variables['lon'][:]\n",
    "print(np.min(lon_regions),np.max(lon_regions)) # lon should be from 0-360 (if it is from -180:180, use transform_lon_coord)\n",
    "\n",
    "# lon in file is -180:180 -> want 0-360, so transform here\n",
    "regions = transform_lon_coord(regions)\n",
    "print(np.min(regions),np.max(regions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Atm_inv1\n",
      "(180, 360, 29)\n",
      "Change sign\n",
      "Load Atm_inv2\n",
      "(180, 360, 29)\n",
      "Change sign\n",
      "Load Atm_inv3\n",
      "(180, 360, 29)\n",
      "Change sign\n",
      "Load Atm_inv4\n",
      "(180, 360, 29)\n",
      "Change sign\n",
      "Load Atm_inv5\n",
      "(180, 360, 29)\n",
      "Change sign\n",
      "Load Atm_inv6\n",
      "(180, 360, 29)\n",
      "Change sign\n",
      "Transform longitude to 0:360\n",
      "Load CCSM-WHOI\n",
      "(38, 180, 360)\n",
      "Load CESM-ETHZ\n",
      "(39, 180, 360)\n",
      "Load CNRM-ESM2-1\n",
      "(39, 180, 360)\n",
      "Load EC-Earth3\n",
      "(39, 180, 360)\n",
      "Load FESOM_REcoM_HR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/u2/c/cnissen/scripts_reccap/annualmean.py:43: RuntimeWarning: Mean of empty slice\n",
      "  var_annualmean[t,:,:] = np.nanmean(datamB, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39, 180, 360)\n",
      "Load FESOM_REcoM_LR\n",
      "(39, 180, 360)\n",
      "Load MOM6-Princeton\n",
      "(39, 180, 360)\n",
      "Load MPIOM-HAMOCC\n",
      "(40, 180, 360)\n",
      "Load MRI-ESM2-1\n",
      "(39, 180, 360)\n",
      "Load NorESM-OC1.2\n",
      "(39, 180, 360)\n",
      "Load ORCA025-GEOMAR\n",
      "(39, 180, 360)\n",
      "Load ORCA1-LIM3-PISCES\n",
      "(39, 180, 360)\n",
      "Load PlankTOM12\n",
      "(39, 180, 360)\n",
      "Load ROMS-SouthernOcean-ETHZ\n",
      "(39, 180, 360)\n",
      "Load OCIM-v2014-CTL\n",
      "Load OCIM-v2021\n",
      "Load AOML_EXTRAT\n",
      "1997-09-15\n",
      "(23, 180, 360)\n",
      "Load CMEMS-LSCE-FFNN\n",
      "(34, 180, 360)\n",
      "Load CSIRML6\n",
      "(34, 180, 360)\n",
      "Load JenaMLS\n",
      "(34, 180, 360)\n",
      "Load JMAMLR\n",
      "(34, 180, 360)\n",
      "Load LDEO-HPD\n",
      "(34, 180, 360)\n",
      "Load NIES-ML3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1528226/243556053.py:109: UserWarning: WARNING: _FillValue not used since it\n",
      "cannot be safely cast to variable data type\n",
      "  data = np.squeeze(ff.variables['fgco2'][:,:,:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 180, 360)\n",
      "Load OceanSODAETHZ\n",
      "(34, 180, 360)\n",
      "Load SOMFFN\n",
      "(34, 180, 360)\n",
      "Load UOEX_Wat20\n",
      "Load BSOSE\n",
      "Change sign\n",
      "Load ECCO-Darwin\n"
     ]
    }
   ],
   "source": [
    "#-----\n",
    "# load data\n",
    "#-----\n",
    "      \n",
    "years_in_file = np.arange(1980,2018+1)\n",
    "\n",
    "#-----\n",
    "# atmospheric inversion\n",
    "#-----\n",
    "flux_atminv = np.nan*np.ones([180,360,len(eval_time),len(data_atminv)])\n",
    "for ii in range(0,len(data_atminv)):\n",
    "    print ('Load '+data_atminv[ii])\n",
    "    ff = Dataset(path_atminv+'GCP2021_inversions_for_RECCAP2_1x1_version1_1_20211122.nc') \n",
    "    # this file contains 6 products!\n",
    "    data = np.squeeze(ff.variables['ocean_flux_NOT_adjusted'][ii,:,:,:]) # 1990-2020, .mean(axis=0)\n",
    "    data = data[:-24,:,:] # kick out 2019 & 2020\n",
    "    data = annualmean(data)\n",
    "    data = np.transpose(data,[1,2,0]) # time is last\n",
    "    print (data.shape)\n",
    "    try: \n",
    "        data[data.mask==True]=np.nan\n",
    "    except: \n",
    "        pass\n",
    "    data[data==0]=np.nan # set land to NaN\n",
    "    flux_atminv[:,:,5:,ii] = data\n",
    "    print ('Change sign')\n",
    "    flux_atminv[:,:,:,ii] = -1*flux_atminv[:,:,:,ii]\n",
    "    ff.close()\n",
    "    del data\n",
    "print ('Transform longitude to 0:360')\n",
    "flux_atminv = transform_lon_coord(flux_atminv)\n",
    "    \n",
    "        \n",
    "#-----\n",
    "# models\n",
    "#-----\n",
    "flux_models = np.nan*np.ones([180,360,len(eval_time),len(models)])\n",
    "for ii in range(0,len(models)):\n",
    "    print ('Load '+models[ii])\n",
    "    ff = Dataset(path_models+models[ii]+'_'+var+'_'+versionID_models[ii]+'/'+\\\n",
    "                    'fgco2_'+models[ii]+'_'+sim+'_1_gr_1980-2018_'+versionID_models[ii]+'.nc')\n",
    "    data = np.squeeze(ff.variables['fgco2'][:,:,:])\n",
    "    data = annualmean(data)\n",
    "    if models[ii] in ['CCSM-WHOI']: # kick out years 1958-1979\n",
    "        data = -1*data[22:,:,:] # flip sign\n",
    "    print (data.shape)\n",
    "    if models[ii] in ['MPIOM-HAMOCC']: # kick out 2019\n",
    "        data = data[:-1,:,:]\n",
    "    try: \n",
    "        data[data.mask==True]=np.nan\n",
    "    except: \n",
    "        pass\n",
    "    data[data==0]=np.nan\n",
    "        \n",
    "    # find position into which to write annual means (depends on years provided in submission)\n",
    "    if models[ii] in ['OCIM-v2014-CTL','CCSM-WHOI']: # these ones stop in 2017\n",
    "        start_ind = np.where(np.asarray(years_in_file)==year1)[0][0]\n",
    "        end_ind   = np.argmin(np.abs(eval_time-2017)) \n",
    "    else:\n",
    "        start_ind = np.where(np.asarray(years_in_file)==year1)[0][0]\n",
    "        end_ind   = np.where(np.asarray(years_in_file)==year2)[0][0]\n",
    "        \n",
    "    data = np.transpose(data,[1,2,0])\n",
    "    if models[ii] in ['OCIM-v2014-CTL','CCSM-WHOI']:\n",
    "        flux_models[:,:,:end_ind+1,ii] = data[:,:,start_ind:]\n",
    "    else: \n",
    "        flux_models[:,:,:,ii] = data[:,:,start_ind:end_ind+1] #np.nanmean(data,axis=0)\n",
    "    ff.close()\n",
    "    del data\n",
    "    \n",
    "#-----\n",
    "# OCIM\n",
    "#-----\n",
    "flux_ocim = np.nan*np.ones([180,360,len(eval_time),len(data_ocim)])\n",
    "for ii in range(0,len(data_ocim)):\n",
    "    print ('Load '+data_ocim[ii])\n",
    "    ff = Dataset(path_models+data_ocim[ii]+'_'+var+'_'+versionID_data_ocim[ii]+'/'+\\\n",
    "                    'fgco2_'+data_ocim[ii]+'_'+sim+'_1_gr_1980-2018_'+versionID_data_ocim[ii]+'.nc')\n",
    "    data = np.squeeze(ff.variables['fgco2'][:,:,:])\n",
    "    data = annualmean(data)\n",
    "    try: \n",
    "        data[data.mask==True]=np.nan\n",
    "    except: \n",
    "        pass\n",
    "    # find position into which to write annual means (depends on years provided in submission)\n",
    "    if data_ocim[ii] in ['OCIM-v2014-CTL']:\n",
    "        start_ind = np.where(np.asarray(years_in_file)==year1)[0][0]\n",
    "        end_ind   = np.argmin(np.abs(eval_time-2017)) \n",
    "    else:\n",
    "        start_ind = np.where(np.asarray(years_in_file)==year1)[0][0]\n",
    "        end_ind   = np.where(np.asarray(years_in_file)==year2)[0][0]\n",
    "        \n",
    "    data = np.transpose(data,[1,2,0])\n",
    "    if data_ocim[ii] in ['OCIM-v2014-CTL']:\n",
    "        flux_ocim[:,:,:end_ind+1,ii] = data[:,:,start_ind:]\n",
    "    else: \n",
    "        flux_ocim[:,:,:,ii] = data[:,:,start_ind:end_ind+1] #np.nanmean(data,axis=0)\n",
    "    ff.close()\n",
    "    del data\n",
    "        \n",
    "#-----\n",
    "# data products\n",
    "#-----\n",
    "flux_data_products = np.nan*np.ones([180,360,len(eval_time),len(data_prod)])\n",
    "for ii in range(0,len(data_prod)):\n",
    "    print ('Load '+data_prod[ii])\n",
    "    ff = Dataset(path_data+data_prod[ii]+'_'+versionID_data_prod[ii]+'/'+\\\n",
    "                    filename_data_prod[ii])\n",
    "    data = np.squeeze(ff.variables['fgco2'][:,:,:])\n",
    "    if data_prod[ii] in ['AOML_RANDOMF','AOML_EXTRAT']: # get starting time of this product \n",
    "        # -> to get rid of incomplete year in beginning\n",
    "        time = np.squeeze(ff.variables['time'][:])\n",
    "        day = time[0]\n",
    "        start = date(1980,1,1)      # This is the \"days since\" part\n",
    "        delta = timedelta(day)     # Create a time delta object from the number of days\n",
    "        offset = start + delta      # Add the specified number of days to 1990\n",
    "        print(offset)               # >>>  2015-12-01\n",
    "        del time,day,start,delta,offset\n",
    "        # delete first 4 entries!\n",
    "        data = data[4:,:,:]\n",
    "        #data = data[:-24,:,:] # kick out 2019 & 2020\n",
    "            \n",
    "    if data_prod[ii] in ['NIES-ML3']: # reorganize dimensions to math the others\n",
    "        data = np.transpose(data,[2,1,0])\n",
    "    if data_prod[ii] in ['SOMFFN']: # reorganize dimensions to math the others\n",
    "        data = np.transpose(data,[2,1,0])\n",
    "        data = data[3*12:,:,:] # kick out 1982-1984\n",
    "    if data_prod[ii] in ['JMAMLR','NIES-ML3']: \n",
    "        data = data[5*12:,:,:] # kick out 1980-1984 (filename suggests it starts in 1985, but timedim suggests otherwise)\n",
    "    if data_prod[ii] in ['JMAMLR','NIES-nn','CSIRML6','SOMFFN']: # 1985-2019\n",
    "        data = data[:-12,:,:] # kick out 2019\n",
    "    if data_prod[ii] in ['NIES-ML3']: # 1985-2020\n",
    "        data = data[:-24,:,:] # kick out 2019 & 2020\n",
    "    if data_prod[ii] in ['NIES-nn','NIES-ML3']: # mask missing values\n",
    "        data[data<-9999999]=np.nan\n",
    "        \n",
    "    data = annualmean(data)\n",
    "    print (data.shape)\n",
    "    data = np.transpose(data,[1,2,0]) # time is last\n",
    "        \n",
    "    if data_prod[ii] in ['AOML_RANDOMF','AOML_EXTRAT']: #1997-2020 in file, but 280 months -> 2020 not complete? \n",
    "        # it is 1998-2020\n",
    "        data = data[:,:,:-2] # kick out 2019 & 2020\n",
    "        years_in_file2 = np.arange(1998,2018+1)\n",
    "        start_ind = np.where(np.asarray(eval_time)==years_in_file2[0])[0][0]\n",
    "        flux_data_products[:,:,start_ind:,ii] = data\n",
    "        del years_in_file2,start_ind\n",
    "    else: \n",
    "        flux_data_products[:,:,:,ii] = data\n",
    "             \n",
    "    # for some products, change sign so that pos=into ocean\n",
    "    if data_prod[ii] in ['NIES-nn']:\n",
    "        print ('Change sign')\n",
    "        flux_data_products[:,:,:,ii] = -1*flux_data_products[:,:,:,ii] \n",
    "    ff.close()\n",
    "    del data\n",
    "\n",
    "#-----\n",
    "# data Watson2020\n",
    "#-----\n",
    "flux_data_watson = np.nan*np.ones([180,360,len(eval_time),len(data_watson)])\n",
    "for ii in range(0,len(data_watson)):\n",
    "    print ('Load '+data_watson[ii])\n",
    "    ff = Dataset(path_data+data_watson[ii]+'_'+versionID_data_watson[ii]+'/'+\\\n",
    "                    filename_data_watson[ii])\n",
    "    ind_start_watson = 12*3 # skip first three years, i.e. 1985-1987 (wind climatology is used)\n",
    "    data = np.squeeze(ff.variables['fgco2'][ind_start_watson:,:,:]) \n",
    "    data = annualmean(data)\n",
    "    data = np.transpose(data,[1,2,0]) # time is last\n",
    "    if data_watson[ii] in ['UOEX_Wat20']: # 1985-2019\n",
    "        data = data[:,:,:-1]\n",
    "        ind_start_watson = 3 # here, only annual\n",
    "        flux_data_watson[:,:,ind_start_watson:,ii] = data\n",
    "    ff.close()\n",
    "    del data\n",
    "     \n",
    "#-----\n",
    "# data-assimilating models \n",
    "#-----\n",
    "flux_data_assim = np.nan*np.ones([180,360,len(eval_time),len(data_assim)])\n",
    "for ii in range(0,len(data_assim)):\n",
    "    print ('Load '+data_assim[ii])\n",
    "    ff = Dataset(path_models+data_assim[ii]+'_'+var+'_'+versionID_data_assim[ii]+'/'+\\\n",
    "                    'fgco2_'+data_assim[ii]+'_'+sim+'_1_gr_1980-2018_'+versionID_data_assim[ii]+'.nc')\n",
    "    data = np.squeeze(ff.variables['fgco2'][:,:,:])\n",
    "    if data_assim[ii] in ['ECCO-Darwin']: # transform longitudes\n",
    "        data = transform_lon_coord(data) \n",
    "    data = annualmean(data)\n",
    "    data = np.transpose(data,[1,2,0]) # time is last\n",
    "    if data_assim[ii] in ['BSOSE']: #2013-2019\n",
    "        data = data[:,:,:-1] # kick out 2019\n",
    "        years_in_file2 = np.arange(2013,2018+1)\n",
    "        start_ind = np.where(np.asarray(eval_time)==years_in_file2[0])[0][0]\n",
    "        flux_data_assim[:,:,start_ind:,ii] = data\n",
    "        del years_in_file2,start_ind\n",
    "    elif data_assim[ii] in ['ECCO-Darwin']: #1985-2018\n",
    "        start_ind = np.argmin(np.abs(eval_time-1995)) \n",
    "        flux_data_assim[:,:,start_ind:,ii] = data[:,:,:]\n",
    "        del start_ind\n",
    "    # for some products, change sign so that pos=into ocean\n",
    "    if data_assim[ii] in ['BSOSE']:\n",
    "        print ('Change sign')\n",
    "        flux_data_assim[:,:,:,ii] = -1*flux_data_assim[:,:,:,ii] \n",
    "    ff.close()\n",
    "    del data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 180, 360) (456, 180, 360)\n",
      "1957-01-15 1982-01-15\n",
      "2020-12-15 2019-12-15\n",
      "(408, 180, 360) (408, 180, 360)\n",
      "(2, 34, 180, 360)\n",
      "(180, 360, 34, 2)\n"
     ]
    }
   ],
   "source": [
    "#----\n",
    "# load SOCCOM files from Seth\n",
    "#----\n",
    "\n",
    "ff = Dataset(path_soccom+'RECCAP_regrid_Jena_CS.nc')\n",
    "soccom1 = ff.variables['F_CO2'][:] \n",
    "time1 = ff.variables['time'][:] # starts in Jan 1957, ends in Dec 2020\n",
    "ff.close()\n",
    "ff = Dataset(path_soccom+'RECCAP_regrid_SOM_FFN.nc')\n",
    "soccom2 = ff.variables['F_CO2'][:]\n",
    "time2 = ff.variables['time'][:] # starts in Jan 1982, ends in Dec 2019\n",
    "ff.close()\n",
    "print (soccom1.shape,soccom2.shape)\n",
    "\n",
    "start = date(1950,1,1)      # This is the \"days since\" part\n",
    "offset1 = start + timedelta(time1[0])      # Add the specified number of days to 1990\n",
    "offset2 = start + timedelta(time2[0])      # Add the specified number of days to 1990\n",
    "print(offset1,offset2) \n",
    "\n",
    "end1 = start + timedelta(time1[-1])      # Add the specified number of days to 1990\n",
    "end2 = start + timedelta(time2[-1])      # Add the specified number of days to 1990\n",
    "print(end1,end2) \n",
    "\n",
    "# reduce data to Jan-1985 until Dec-2018\n",
    "soccom1 = soccom1[(28)*12:-24,:,:]\n",
    "soccom2 = soccom2[(3)*12:-12,:,:]\n",
    "print (soccom1.shape,soccom2.shape )\n",
    "\n",
    "# avg over the chosen season\n",
    "soccom1 = annualmean(-1*soccom1) # CHANGE SIGN\n",
    "soccom2 = annualmean(-1*soccom2)\n",
    "\n",
    "soccom_all = np.stack((soccom1,soccom2))\n",
    "print(soccom_all.shape)\n",
    "\n",
    "#np.nan*np.ones([180,360,len(eval_time),len(data_assim)])\n",
    "flux_soccom = np.transpose(soccom_all,[2,3,1,0])\n",
    "print (flux_soccom.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOCCOM_Jena: -1.9408281820965678e-07 3.408697509806508e-07\n",
      "SOCCOM_SOMFFN: -1.8549687532292555e-07 2.2637804079616248e-07\n",
      "CCSM-WHOI: -3.328032391891611e-07 2.3799766779575293e-07\n",
      "CESM-ETHZ: -1.8722379024893598e-07 1.8758713338229427e-07\n",
      "CNRM-ESM2-1: -3.473808677055083e-07 7.243008834001572e-07\n",
      "EC-Earth3: -3.455088517512195e-07 2.9743890195277345e-07\n",
      "FESOM_REcoM_HR: -1.0287609477898954e-06 3.291037252258339e-07\n",
      "FESOM_REcoM_LR: -5.16851747625652e-07 3.551971909955131e-07\n",
      "MOM6-Princeton: -6.316397609341927e-07 8.285434885537167e-07\n",
      "MPIOM-HAMOCC: -2.1326543730992853e-07 2.601472033347818e-07\n",
      "MRI-ESM2-1: -6.728191124238947e-07 2.867927548777516e-07\n",
      "NorESM-OC1.2: -3.108359862835641e-07 3.516230151490163e-07\n",
      "ORCA025-GEOMAR: -5.994956284336938e-07 3.7668123493913067e-07\n",
      "ORCA1-LIM3-PISCES: -4.1642874748504255e-07 2.7747370268116356e-07\n",
      "PlankTOM12: -4.364177641491551e-07 1.7493054826900334e-07\n",
      "ROMS-SouthernOcean-ETHZ: -2.626861999033281e-07 4.0850900973055104e-07\n",
      "OCIM-v2014-CTL: -5.438138318188809e-07 3.9715005304468195e-07\n",
      "OCIM-v2021: -1.6344802528768662e-07 2.887194862035919e-07\n",
      "AOML_EXTRAT: -2.3505680246671545e-07 2.589572716260591e-07\n",
      "CMEMS-LSCE-FFNN: -2.147465210100563e-07 2.4084329197648913e-07\n",
      "CSIRML6: -2.3227200074416032e-07 2.2513133046686562e-07\n",
      "JenaMLS: -2.313928888497685e-07 3.408913471503183e-07\n",
      "JMAMLR: -3.523176701492048e-07 4.76805922744461e-07\n",
      "LDEO-HPD: -2.0642848733611228e-07 9.681677593093457e-06\n",
      "NIES-ML3: -1.3827569489421876e-07 2.2196444149358285e-07\n",
      "OceanSODAETHZ: -2.0727945128431633e-07 2.144097677243522e-07\n",
      "SOMFFN: -1.5189935709258862e-07 2.2940702137930202e-07\n",
      "UOEX_Wat20: -3.8475423226074856e-07 2.7657974721178115e-07\n",
      "BSOSE: -1.8456413450691586e-07 1.385278215329406e-07\n",
      "ECCO-Darwin: -2.499237439224089e-07 2.595669741367601e-07\n",
      "Atm_inv1: -6.063982676348292e-14 7.428602910783358e-14\n",
      "Atm_inv2: -7.350243582162802e-14 1.2911138674752526e-13\n",
      "Atm_inv3: -1.7550611192640003e-13 4.819990884351766e-13\n",
      "Atm_inv4: -1.277087153721271e-13 1.6808744555519472e-13\n",
      "Atm_inv5: -8.656837261916885e-14 1.6077319427750935e-13\n",
      "Atm_inv6: -4.46287893858246e-14 6.434789258075774e-14\n"
     ]
    }
   ],
   "source": [
    "#------\n",
    "# print some numbers (to spot any obvious unit problems)\n",
    "#------\n",
    "\n",
    "for ii in range(0,len(soccom)):\n",
    "    print (soccom[ii]+': '+str(np.nanmin(flux_soccom[:,:,:,ii]))+' '+str(np.nanmax(flux_soccom[:,:,:,ii])))\n",
    "        \n",
    "for ii in range(0,len(models)):\n",
    "    print (models[ii]+': '+str(np.nanmin(flux_models[:,:,:,ii]))+' '+str(np.nanmax(flux_models[:,:,:,ii])))\n",
    "\n",
    "for ii in range(0,len(data_ocim)):\n",
    "    print (data_ocim[ii]+': '+str(np.nanmin(flux_ocim[:,:,:,ii]))+' '+str(np.nanmax(flux_ocim[:,:,:,ii])))\n",
    "\n",
    "for ii in range(0,len(data_prod)):\n",
    "    print (data_prod[ii]+': '+str(np.nanmin(flux_data_products[:,:,:,ii]))+' '+str(np.nanmax(flux_data_products[:,:,:,ii])))\n",
    "\n",
    "for ii in range(0,len(data_watson)):\n",
    "    print (data_watson[ii]+': '+str(np.nanmin(flux_data_watson[:,:,:,ii]))+' '+str(np.nanmax(flux_data_watson[:,:,:,ii])))\n",
    "\n",
    "for ii in range(0,len(data_assim)):\n",
    "    print (data_assim[ii]+': '+str(np.nanmin(flux_data_assim[:,:,:,ii]))+' '+str(np.nanmax(flux_data_assim[:,:,:,ii])))\n",
    "\n",
    "for ii in range(0,len(data_atminv)):\n",
    "    print (data_atminv[ii]+': '+str(np.nanmin(flux_atminv[:,:,:,ii]))+' '+str(np.nanmax(flux_atminv[:,:,:,ii])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50% done\n",
      "Global area: 509364377985322.94 m2\n",
      "Total Southern Ocean ocean surface area 77409028462219.4 m2\n",
      "TEST: the following three should be identical\n",
      "77409028462219.4\n",
      "77409028462219.42\n",
      "77409028462219.42\n"
     ]
    }
   ],
   "source": [
    "#------\n",
    "# get surface area of all the biomes\n",
    "#------\n",
    "# calculate area with sw.dist\n",
    "\n",
    "def get_areas_biomes(area,regions,subregions):\n",
    "    # calculate area-weighted averages of a given quantity in data according to biomes defined\n",
    "    # after Fay & McKinley (2014), use RECCAP mask loaded further up\n",
    "    # provide \"area\" (with lon from 0:360)\n",
    "    \n",
    "    # only consider area for SO RECCAP mask\n",
    "    area[regions.mask==True]=0\n",
    "    print ('Total Southern Ocean ocean surface area '+str(np.nansum(area))+' m2')\n",
    "    \n",
    "    data_avg = np.zeros(13) # 9 regions in SO, + 3 for combined sectors, +1 for whole SO\n",
    "    for i in range(0,13):\n",
    "        reg2  = regions.ravel()\n",
    "        area2 = area.ravel()\n",
    "        if subregions[i] in ['STSS-Atl']:\n",
    "            ind_region = np.where(reg2==0)[0]\n",
    "        elif subregions[i] in ['STSS-Ind']:\n",
    "            ind_region = np.where(reg2==3)[0]\n",
    "        elif subregions[i] in ['STSS-Pac']:\n",
    "            ind_region = np.where(reg2==6)[0]\n",
    "        elif subregions[i] in ['SPSS-Atl']:\n",
    "            ind_region = np.where(reg2==1)[0]\n",
    "        elif subregions[i] in ['SPSS-Ind']:\n",
    "            ind_region = np.where(reg2==4)[0]\n",
    "        elif subregions[i] in ['SPSS-Pac']:\n",
    "            ind_region = np.where(reg2==7)[0]\n",
    "        elif subregions[i] in ['ICE-Atl']:\n",
    "            ind_region = np.where(reg2==2)[0]\n",
    "        elif subregions[i] in ['ICE-Ind']:\n",
    "            ind_region = np.where(reg2==5)[0]\n",
    "        elif subregions[i] in ['ICE-Pac']:\n",
    "            ind_region = np.where(reg2==8)[0]\n",
    "        elif subregions[i] in ['STSS']:\n",
    "            ind_region = np.where((reg2==0) | (reg2==3) | (reg2==6))[0]\n",
    "        elif subregions[i] in ['SPSS']:\n",
    "            ind_region = np.where((reg2==1) | (reg2==4) | (reg2==7))[0]\n",
    "        elif subregions[i] in ['ICE']:\n",
    "            ind_region =np.where((reg2==2) | (reg2==5) | (reg2==8))[0]\n",
    "        elif subregions[i] in ['all']:\n",
    "            ind_region = np.where(reg2>=0)[0]\n",
    "        data_avg[i] = np.nansum(area2[ind_region]) #np.nansum(data2[ind_region]*area2[ind_region]/total_area)\n",
    "        del reg2, area2,ind_region\n",
    "    return data_avg\n",
    "\n",
    "# define for filtering, but don't use for area calculation\n",
    "lon = np.arange(0.5,359.5+1,1)\n",
    "lat = np.arange(-89.5,89.5+1,1)\n",
    "\n",
    "# need the box boundaries for area calculation\n",
    "xi = np.arange(0,360+1,1)\n",
    "yi = np.arange(-90,90+1,1) \n",
    "\n",
    "area = np.zeros((len(xi)-1,len(yi)-1))\n",
    "#calculate area\n",
    "for i in range(0,len(xi)-1): #laenge pruefen!\n",
    "    if i==np.round(len(xi)/2): \n",
    "        print ('50% done')\n",
    "    for j in range(0,len(yi)-1): \n",
    "        dist1 = sw.dist([yi[j],yi[j+1]],[xi[i],xi[i]])\n",
    "        dist2 = sw.dist([yi[j],yi[j]],[xi[i],xi[i+1]])\n",
    "        area[i][j] = float(dist1[0]) * float(dist2[0]) *1000 *1000 #m2\n",
    "area = area.transpose()\n",
    "print ('Global area:',np.sum(area),'m2') \n",
    "area_global = np.copy(area)\n",
    "\n",
    "area_biomes = get_areas_biomes(area,regions,subregions)\n",
    "\n",
    "#print (area_biomes)\n",
    "print('TEST: the following three should be identical')\n",
    "print (np.sum(area_biomes[0:9])) # 9 subregions\n",
    "print (np.sum(area_biomes[9:12])) # 3 subregions\n",
    "print (np.sum(area_biomes[12])) # 1 subregion \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----\n",
    "# get subareas avg\n",
    "#-----\n",
    "# NOTE: for each product/model, get areas of biomes separately\n",
    "# the areas are not the same for the different prodcuts\n",
    "\n",
    "def get_subarea_avg(flux_models,regions,area,eval_time,models_ABCD,subregions):\n",
    "    flux_subareas  = np.nan*np.ones([len(eval_time),len(models_ABCD),len(subregions)]) \n",
    "    biome_areas    = np.nan*np.ones([len(models_ABCD),len(subregions)]) \n",
    "    for pp in range(0,len(models_ABCD)):\n",
    "        counter=0 # only for printing of total area\n",
    "        print ('Process '+models_ABCD[pp])\n",
    "        for i in range(0,len(subregions)):\n",
    "            for yy in range(0,len(eval_time)):\n",
    "                data1 = flux_models[:,:,yy,pp]\n",
    "                reg2  = regions.ravel()  # SO RECCAP mask\n",
    "                area2 = area.ravel() # surface area\n",
    "                data2 = data1.ravel()\n",
    "                if subregions[i] in ['STSS-Atl']:\n",
    "                    ind_region = np.where(reg2==0)[0]\n",
    "                elif subregions[i] in ['STSS-Ind']:\n",
    "                    ind_region = np.where(reg2==3)[0]\n",
    "                elif subregions[i] in ['STSS-Pac']:\n",
    "                    ind_region = np.where(reg2==6)[0]\n",
    "                elif subregions[i] in ['SPSS-Atl']:\n",
    "                    ind_region = np.where(reg2==1)[0]\n",
    "                elif subregions[i] in ['SPSS-Ind']:\n",
    "                    ind_region = np.where(reg2==4)[0]\n",
    "                elif subregions[i] in ['SPSS-Pac']:\n",
    "                    ind_region = np.where(reg2==7)[0]\n",
    "                elif subregions[i] in ['ICE-Atl']:\n",
    "                    ind_region = np.where(reg2==2)[0]\n",
    "                elif subregions[i] in ['ICE-Ind']:\n",
    "                    ind_region = np.where(reg2==5)[0]\n",
    "                elif subregions[i] in ['ICE-Pac']:\n",
    "                    ind_region = np.where(reg2==8)[0]\n",
    "                elif subregions[i] in ['STSS']:\n",
    "                    ind_region = np.where((reg2==0) | (reg2==3) | (reg2==6))[0]\n",
    "                elif subregions[i] in ['SPSS']:\n",
    "                    ind_region = np.where((reg2==1) | (reg2==4) | (reg2==7))[0]\n",
    "                elif subregions[i] in ['ICE']:\n",
    "                    ind_region =np.where((reg2==2) | (reg2==5) | (reg2==8))[0]\n",
    "                elif subregions[i] in ['all']:\n",
    "                    ind_region = np.where(reg2>=0)[0]\n",
    "                    \n",
    "                ind_not_nan_data = np.where(~np.isnan(data2[ind_region]))[0] # only consider data points that are filled\n",
    "                total_area = np.sum(area2[ind_region][ind_not_nan_data])\n",
    "                #total_area = np.sum(area2[ind_region])\n",
    "                if np.nansum(data1)!=0: # only for years which are filled\n",
    "                    if (subregions[i] in ['all']) & (counter==0):\n",
    "                        print (total_area )\n",
    "                        counter = 1\n",
    "                    biome_areas[pp,i] = total_area \n",
    "                    flux_subareas[yy,pp,i] = np.nansum(data2[ind_region]*area2[ind_region]/total_area)\n",
    "                del data1,reg2,area2,data2,ind_region,total_area\n",
    "    return flux_subareas,biome_areas\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "data products\n",
      "-----\n",
      "Process AOML_EXTRAT\n",
      "77182686197228.12\n",
      "Process CMEMS-LSCE-FFNN\n",
      "73647074238260.6\n",
      "Process CSIRML6\n",
      "75956069483462.88\n",
      "Process JenaMLS\n",
      "77405832654098.36\n",
      "Process JMAMLR\n",
      "72001325799725.03\n",
      "Process LDEO-HPD\n",
      "77043201248352.36\n",
      "Process NIES-ML3\n",
      "76336956437489.22\n",
      "Process OceanSODAETHZ\n",
      "76482430896315.38\n",
      "Process SOMFFN\n",
      "74444375365152.88\n"
     ]
    }
   ],
   "source": [
    "#-----\n",
    "# get subareas avg: data products\n",
    "#-----\n",
    "# NOTE: for each product/model, get areas of biomes separately\n",
    "# the areas are not the same for the different prodcuts\n",
    "def get_subarea_avg_TEST(flux_models,regions,area,eval_time,models_ABCD,subregions):\n",
    "    biome_areas    = np.nan*np.ones([len(eval_time),len(models_ABCD),len(subregions)]) \n",
    "    for pp in range(0,len(models_ABCD)):\n",
    "        counter=0 # only for printing of total area\n",
    "        print ('Process '+models_ABCD[pp])\n",
    "        for i in range(0,len(subregions)):\n",
    "            for yy in range(0,len(eval_time)):\n",
    "                data1 = flux_models[:,:,yy,pp]\n",
    "                reg2  = regions.ravel()  # SO RECCAP mask\n",
    "                area2 = area.ravel() # surface area\n",
    "                data2 = data1.ravel()\n",
    "                if subregions[i] in ['STSS-Atl']:\n",
    "                    ind_region = np.where(reg2==0)[0]\n",
    "                elif subregions[i] in ['STSS-Ind']:\n",
    "                    ind_region = np.where(reg2==3)[0]\n",
    "                elif subregions[i] in ['STSS-Pac']:\n",
    "                    ind_region = np.where(reg2==6)[0]\n",
    "                elif subregions[i] in ['SPSS-Atl']:\n",
    "                    ind_region = np.where(reg2==1)[0]\n",
    "                elif subregions[i] in ['SPSS-Ind']:\n",
    "                    ind_region = np.where(reg2==4)[0]\n",
    "                elif subregions[i] in ['SPSS-Pac']:\n",
    "                    ind_region = np.where(reg2==7)[0]\n",
    "                elif subregions[i] in ['ICE-Atl']:\n",
    "                    ind_region = np.where(reg2==2)[0]\n",
    "                elif subregions[i] in ['ICE-Ind']:\n",
    "                    ind_region = np.where(reg2==5)[0]\n",
    "                elif subregions[i] in ['ICE-Pac']:\n",
    "                    ind_region = np.where(reg2==8)[0]\n",
    "                elif subregions[i] in ['STSS']:\n",
    "                    ind_region = np.where((reg2==0) | (reg2==3) | (reg2==6))[0]\n",
    "                elif subregions[i] in ['SPSS']:\n",
    "                    ind_region = np.where((reg2==1) | (reg2==4) | (reg2==7))[0]\n",
    "                elif subregions[i] in ['ICE']:\n",
    "                    ind_region =np.where((reg2==2) | (reg2==5) | (reg2==8))[0]\n",
    "                elif subregions[i] in ['all']:\n",
    "                    ind_region = np.where(reg2>=0)[0]\n",
    "\n",
    "                ind_not_nan_data = np.where(~np.isnan(data2[ind_region]))[0] # only consider data points that are filled\n",
    "                total_area = np.sum(area2[ind_region][ind_not_nan_data])\n",
    "                #total_area = np.sum(area2[ind_region])\n",
    "                if np.nansum(data1)!=0: # only for years which are filled\n",
    "                    if (subregions[i] in ['all']) & (counter==0):\n",
    "                        print (total_area )\n",
    "                        counter = 1\n",
    "                    biome_areas[yy,pp,i] = total_area \n",
    "                del data1,reg2,area2,data2,ind_region,total_area\n",
    "    return biome_areas\n",
    "\n",
    "print ('-----')\n",
    "print ('data products')\n",
    "print ('-----')\n",
    "biome_area_data_products_varying = get_subarea_avg_TEST(flux_data_products,regions,area,\\\n",
    "                                                    eval_time,data_prod,subregions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "SOCCOM\n",
      "-----\n",
      "Process SOCCOM_Jena\n",
      "76893579241079.84\n",
      "Process SOCCOM_SOMFFN\n",
      "74444375365152.88\n",
      "-----\n",
      "models\n",
      "-----\n",
      "Process CCSM-WHOI\n",
      "74492275264407.1\n",
      "Process CESM-ETHZ\n",
      "76149904481959.67\n",
      "Process CNRM-ESM2-1\n",
      "77371852756049.22\n",
      "Process EC-Earth3\n",
      "77367548050317.14\n",
      "Process FESOM_REcoM_HR\n",
      "77075005194060.84\n",
      "Process FESOM_REcoM_LR\n",
      "77075005194060.84\n",
      "Process MOM6-Princeton\n",
      "77075005194060.84\n",
      "Process MPIOM-HAMOCC\n",
      "76003285884316.31\n",
      "Process MRI-ESM2-1\n",
      "77325948214566.8\n",
      "Process NorESM-OC1.2\n",
      "77374505093557.94\n",
      "Process ORCA025-GEOMAR\n",
      "77289199561810.75\n",
      "Process ORCA1-LIM3-PISCES\n",
      "77402636845977.3\n",
      "Process PlankTOM12\n",
      "76670454088793.06\n",
      "Process ROMS-SouthernOcean-ETHZ\n",
      "76894536194319.66\n",
      "-----\n",
      "OCIM\n",
      "-----\n",
      "Process OCIM-v2014-CTL\n",
      "77075005194060.84\n",
      "Process OCIM-v2021\n",
      "77075005194060.84\n",
      "-----\n",
      "data products\n",
      "-----\n",
      "Process AOML_EXTRAT\n",
      "77182686197228.12\n",
      "Process CMEMS-LSCE-FFNN\n",
      "73647074238260.6\n",
      "Process CSIRML6\n",
      "75956069483462.88\n",
      "Process JenaMLS\n",
      "77405832654098.36\n",
      "Process JMAMLR\n",
      "72001325799725.03\n",
      "Process LDEO-HPD\n",
      "77043201248352.36\n",
      "Process NIES-ML3\n",
      "76336956437489.22\n",
      "Process OceanSODAETHZ\n",
      "76482430896315.38\n",
      "Process SOMFFN\n",
      "74444375365152.88\n",
      "-----\n",
      "data products/Watson2020\n",
      "-----\n",
      "Process UOEX_Wat20\n",
      "77282868108702.88\n",
      "-----\n",
      "data-assimilating models\n",
      "-----\n",
      "Process BSOSE\n",
      "77409028462219.42\n",
      "Process ECCO-Darwin\n",
      "77391956676361.11\n",
      "-----\n",
      "atm inversions\n",
      "-----\n",
      "Process Atm_inv1\n",
      "77018759944804.48\n",
      "Process Atm_inv2\n",
      "77390877297718.98\n",
      "Process Atm_inv3\n",
      "76892744879585.9\n",
      "Process Atm_inv4\n",
      "76051348360719.22\n",
      "Process Atm_inv5\n",
      "75948238127965.08\n",
      "Process Atm_inv6\n",
      "77409028462219.42\n"
     ]
    }
   ],
   "source": [
    "#------\n",
    "# get subarea mean of models, data product, data assimilation products\n",
    "#------\n",
    "\n",
    "# calculate subarea averages\n",
    "#subregions = ('STSS_Atl','SPSS_Atl','ICE_Atl','STSS_Ind','SPSS_Ind',\\\n",
    "#              'ICE_Ind','STSS_Pac','SPSS_Pac','ICE_Pac','STSS','SPSS','ICE')#,'all')\n",
    "# 13 regions: 9 regions in SO, + 3 for combined sectors, +1 for whole SO\n",
    "\n",
    "print ('-----')\n",
    "print ('SOCCOM')\n",
    "print ('-----')\n",
    "flux_soccom_subareas,biome_area_soccom = get_subarea_avg(flux_soccom,regions,area,\\\n",
    "                                                eval_time,soccom,subregions)\n",
    "    \n",
    "print ('-----')\n",
    "print ('models')\n",
    "print ('-----')\n",
    "flux_models_subareas,biome_area_models = get_subarea_avg(flux_models,regions,area,\\\n",
    "                                                eval_time,models,subregions)\n",
    "print ('-----')\n",
    "print ('OCIM')\n",
    "print ('-----')\n",
    "flux_ocim_subareas,biome_area_ocim = get_subarea_avg(flux_ocim,regions,area,\\\n",
    "                                                eval_time,data_ocim,subregions)\n",
    "        \n",
    "print ('-----')\n",
    "print ('data products')\n",
    "print ('-----')\n",
    "flux_data_products_subareas,biome_area_data_products = get_subarea_avg(flux_data_products,regions,area,\\\n",
    "                                                eval_time,data_prod,subregions)\n",
    "\n",
    "print ('-----')\n",
    "print ('data products/Watson2020')\n",
    "print ('-----')\n",
    "flux_data_watson_subareas,biome_area_data_watson = get_subarea_avg(flux_data_watson,regions,area,\\\n",
    "                                                eval_time,data_watson,subregions)\n",
    "\n",
    "print ('-----')\n",
    "print ('data-assimilating models')\n",
    "print ('-----')\n",
    "flux_data_assim_subareas,biome_area_data_assim = get_subarea_avg(flux_data_assim,regions,area,\\\n",
    "                                                eval_time,data_assim,subregions)\n",
    "\n",
    "print ('-----')\n",
    "print ('atm inversions')\n",
    "print ('-----')\n",
    "flux_atminv_subareas,biome_area_atminv = get_subarea_avg(flux_atminv,regions,area,\\\n",
    "                                                eval_time,data_atminv,subregions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######\n",
      "Subregion all:\n",
      "######\n",
      "SOCCOM_Jena: 1.5839072879459185e-08\n",
      "SOCCOM_SOMFFN: 2.197143579812024e-08\n",
      "CCSM-WHOI: 2.035881093091796e-08\n",
      "CESM-ETHZ: 3.120886545477298e-08\n",
      "CNRM-ESM2-1: 1.842900355708883e-08\n",
      "EC-Earth3: 1.5997418995185397e-08\n",
      "FESOM_REcoM_HR: 4.683412766973994e-08\n",
      "FESOM_REcoM_LR: 3.871533671451623e-08\n",
      "MOM6-Princeton: 3.9561852176573295e-08\n",
      "MPIOM-HAMOCC: 4.1949297658834164e-08\n",
      "MRI-ESM2-1: 3.443743117206353e-08\n",
      "NorESM-OC1.2: 4.3581461856629926e-08\n",
      "ORCA025-GEOMAR: 4.581613858215737e-08\n",
      "ORCA1-LIM3-PISCES: 2.985258511197887e-08\n",
      "PlankTOM12: 2.95879817034716e-08\n",
      "ROMS-SouthernOcean-ETHZ: 3.5841547686978557e-08\n",
      "OCIM-v2014-CTL: 5.340414076981168e-08\n",
      "OCIM-v2021: 5.37501090560348e-08\n",
      "AOML_EXTRAT: 4.166614991545029e-08\n",
      "CMEMS-LSCE-FFNN: 3.955209097029648e-08\n",
      "CSIRML6: 3.4891743188601984e-08\n",
      "JenaMLS: 3.750394673667549e-08\n",
      "JMAMLR: 4.2990999302987564e-08\n",
      "LDEO-HPD: 2.9882843521484294e-08\n",
      "NIES-ML3: 2.9262687758036157e-08\n",
      "OceanSODAETHZ: 3.632073182837807e-08\n",
      "SOMFFN: 2.9514761965870207e-08\n",
      "UOEX_Wat20: 4.3062437816036666e-08\n",
      "BSOSE: 2.363341090521747e-08\n",
      "ECCO-Darwin: 4.786363303725559e-08\n",
      "Atm_inv1: 1.1831145100102035e-14\n",
      "Atm_inv2: 1.4101525432475278e-14\n",
      "Atm_inv3: 1.1252967798258506e-14\n",
      "Atm_inv4: 9.645504271717868e-15\n",
      "Atm_inv5: 1.0120014908631622e-14\n",
      "Atm_inv6: 1.008701315331835e-14\n"
     ]
    }
   ],
   "source": [
    "#--- \n",
    "# print some numbers\n",
    "#---\n",
    "\n",
    "ss = -1\n",
    "\n",
    "print ('######')\n",
    "print ('Subregion '+subregions[ss]+':')\n",
    "print ('######')\n",
    "\n",
    "for ii in range(0,len(soccom)):\n",
    "        print (soccom[ii]+': '+str(flux_soccom_subareas[-2,ii,ss]))\n",
    "                \n",
    "for ii in range(0,len(models)):\n",
    "        if models[ii] in ['OCIM-v2014-CTL','CCSM-WHOI']: # year 2018 not available!, print year 2017!\n",
    "             print (models[ii]+': '+str(flux_models_subareas[-2,ii,ss])) # last year, whole SO \n",
    "        else:\n",
    "            print (models[ii]+': '+str(flux_models_subareas[-1,ii,ss])) # last year, whole SO \n",
    "            \n",
    "for ii in range(0,len(data_ocim)):\n",
    "        if data_ocim[ii] in ['OCIM-v2014-CTL']: # year 2018 not available!, print year 2017!\n",
    "             print (data_ocim[ii]+': '+str(flux_ocim_subareas[-2,ii,ss])) # last year, whole SO \n",
    "        else:\n",
    "            print (data_ocim[ii]+': '+str(flux_ocim_subareas[-1,ii,ss])) # last year, whole SO \n",
    "\n",
    "for ii in range(0,len(data_prod)):\n",
    "        print (data_prod[ii]+': '+str(flux_data_products_subareas[-1,ii,ss]))# last year, whole SO \n",
    "    \n",
    "for ii in range(0,len(data_watson)):\n",
    "        print (data_watson[ii]+': '+str(flux_data_watson_subareas[-1,ii,ss]))\n",
    "        \n",
    "for ii in range(0,len(data_assim)):\n",
    "        print (data_assim[ii]+': '+str(flux_data_assim_subareas[-1,ii,ss]))# last year, whole SO \n",
    "\n",
    "for ii in range(0,len(data_atminv)):\n",
    "        print (data_atminv[ii]+': '+str(flux_atminv_subareas[-1,ii,ss]))# last year, whole SO \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 14, 13)\n",
      "(34, 2, 13)\n",
      "(34, 9, 13)\n",
      "(34, 1, 13)\n",
      "(34, 2, 13)\n",
      "(34, 3, 13)\n",
      "('AOML_EXTRAT', 'CMEMS-LSCE-FFNN', 'CSIRML6', 'JenaMLS', 'JMAMLR', 'LDEO-HPD', 'NIES-ML3', 'OceanSODAETHZ', 'SOMFFN')\n",
      "AOML (all): [           nan            nan            nan            nan\n",
      "            nan            nan            nan            nan\n",
      "            nan            nan            nan            nan\n",
      "            nan 1.40615694e-08 1.52688026e-08 1.65644182e-08\n",
      " 1.63762934e-08 1.97764252e-08 2.42505902e-08 2.51150112e-08\n",
      " 2.91051833e-08 2.72589266e-08 2.46552996e-08 2.61350050e-08\n",
      " 3.06001597e-08 3.55619885e-08 3.86187106e-08 3.47239339e-08\n",
      " 3.52279716e-08 3.66856423e-08 3.68788035e-08 3.77060392e-08\n",
      " 3.90201101e-08 4.16661499e-08]\n",
      "CCSM-WHOI (all): [8.59687351e-09 9.92957096e-09 8.48138119e-09 1.18567546e-08\n",
      " 7.60841138e-09 1.14097375e-08 1.27120112e-08 1.71361824e-08\n",
      " 1.07665565e-08 1.11882030e-08 1.22975347e-08 1.36141723e-08\n",
      " 1.02441266e-08 1.02380263e-08 9.89095468e-09 1.25992091e-08\n",
      " 1.20049642e-08 1.68049253e-08 1.60680548e-08 1.50743157e-08\n",
      " 1.78832654e-08 1.49779431e-08 1.74093184e-08 1.11316513e-08\n",
      " 1.83167299e-08 1.48368310e-08 1.85806082e-08 1.90019672e-08\n",
      " 2.11535253e-08 1.60828375e-08 1.33160988e-08 1.95952961e-08\n",
      " 2.03588109e-08            nan]\n",
      "area_mean_models (all): 76826225858446.95\n",
      "area_mean_data_products (all): 76026296684885.84\n",
      "area_mean_watson (all): 76497425349612.42\n",
      "area_mean_data_assim (all): 77400492569290.27\n",
      "area_mean_ocim (all): 77075005194060.84\n",
      "area_mean_atminv (all): 76821668535432.52\n",
      "area_mean_soccom (all): 75668977303116.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1528226/1738166859.py:17: RuntimeWarning: Mean of empty slice\n",
      "  multi_data_watson_mean= np.nanmean(flux_data_watson_subareas,axis=1)\n",
      "/tmp/ipykernel_1528226/1738166859.py:18: RuntimeWarning: Mean of empty slice\n",
      "  multi_data_assim_mean = np.nanmean(flux_data_assim_subareas,axis=1)\n",
      "/tmp/ipykernel_1528226/1738166859.py:19: RuntimeWarning: Mean of empty slice\n",
      "  multi_atminv_mean     = np.nanmean(flux_atminv_subareas[:,ind_list,:],axis=1) # only include those that start in 1990\n"
     ]
    }
   ],
   "source": [
    "#-----\n",
    "# get multi-model and multi-data mean\n",
    "#-----\n",
    "\n",
    "print (flux_models_subareas.shape)\n",
    "print (flux_ocim_subareas.shape)\n",
    "print (flux_data_products_subareas.shape)\n",
    "print (flux_data_watson_subareas.shape)\n",
    "print (flux_data_assim_subareas.shape)\n",
    "\n",
    "ind_list = [0,1,3] # only include inv1, inv2, inv4 here -> only these start in 1990\n",
    "print (flux_atminv_subareas[:,ind_list,:].shape)\n",
    "\n",
    "multi_model_mean      = np.nanmean(flux_models_subareas,axis=1)\n",
    "multi_ocim_mean       = np.nanmean(flux_ocim_subareas,axis=1)\n",
    "multi_data_prod_mean  = np.nanmean(flux_data_products_subareas,axis=1)\n",
    "multi_data_watson_mean= np.nanmean(flux_data_watson_subareas,axis=1)\n",
    "multi_data_assim_mean = np.nanmean(flux_data_assim_subareas,axis=1)\n",
    "multi_atminv_mean     = np.nanmean(flux_atminv_subareas[:,ind_list,:],axis=1) # only include those that start in 1990\n",
    "multi_soccom_mean     = np.nanmean(flux_soccom_subareas,axis=1)\n",
    "\n",
    "#multi_data_prod_mean  = np.nanmean(flux_data_products[:,:,1:],axis=2) # leave out AOML_EXTRAT (starts later)\n",
    "    \n",
    "print (data_prod)\n",
    "print ('AOML (all):',flux_data_products_subareas[:,0,-1])\n",
    "print ('CCSM-WHOI (all):',flux_models_subareas[:,0,-1])\n",
    "    \n",
    "# get mean area for each product class\n",
    "#print biome_area_atminv.shape\n",
    "area_mean_models        = np.mean(biome_area_models[:,:],axis=0) \n",
    "area_mean_data_products = np.mean(biome_area_data_products[:,:],axis=0) \n",
    "area_mean_watson        = np.mean(biome_area_data_watson[:,:],axis=0)\n",
    "area_mean_data_assim    = np.mean(biome_area_data_assim[:,:],axis=0)\n",
    "area_mean_ocim          = np.mean(biome_area_ocim[:,:],axis=0)\n",
    "area_mean_atminv        = np.mean(biome_area_atminv[ind_list,:],axis=0) # only include those that start in 1990\n",
    "area_mean_soccom        = np.mean(biome_area_soccom[:,:],axis=0)\n",
    "\n",
    "print ('area_mean_models (all):',area_mean_models[-1])\n",
    "print ('area_mean_data_products (all):',area_mean_data_products[-1])\n",
    "print ('area_mean_watson (all):',area_mean_watson[-1])\n",
    "print ('area_mean_data_assim (all):',area_mean_data_assim[-1])\n",
    "print ('area_mean_ocim (all):',area_mean_ocim[-1])\n",
    "print ('area_mean_atminv (all):',area_mean_atminv[-1])\n",
    "print ('area_mean_soccom (all):',area_mean_soccom[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180, 360)\n",
      "Process river file \n",
      "76118473462222.84\n",
      "River fluxes to add for subregion all: 1.3662420531327198e-09\n",
      "\n",
      "River flux adjustment (pos=outgassing):\n",
      "STSS-Atl: -0.01546799211558836 Pg C yr-1\n",
      "STSS-Ind: 0.0026367052644284345 Pg C yr-1\n",
      "STSS-Pac: -0.0013755711900539146 Pg C yr-1\n",
      "SPSS-Atl: -0.008262337193420477 Pg C yr-1\n",
      "SPSS-Ind: 0.045257119271516585 Pg C yr-1\n",
      "SPSS-Pac: 0.016270984555087874 Pg C yr-1\n",
      "ICE-Atl: -0.0035661499261086895 Pg C yr-1\n",
      "ICE-Ind: 0.0054982502435766975 Pg C yr-1\n",
      "ICE-Pac: -0.0015724400214355222 Pg C yr-1\n",
      "STSS: -0.014206858041213835 Pg C yr-1\n",
      "SPSS: 0.05326576663318398 Pg C yr-1\n",
      "ICE: 0.00035966029603248497 Pg C yr-1\n",
      "all: 0.03941856888800264 Pg C yr-1\n",
      "based on STSS/SPSS/ICE: 0.03941856888800263 Pg C yr-1\n",
      "Global ocean area: 509364377985322.94 m2\n",
      "Global river flux: 0.6158395517429716 Pg C yr-1\n",
      "SO/Global: 6.400785525456877%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#----\n",
    "# load river file, apply to model fields\n",
    "#----\n",
    "\n",
    "river_adjustment = True\n",
    "if river_adjustment: \n",
    "    river_adj_names = ('Lacroix2020')\n",
    "    ff = Dataset(path_river+'fgco2_lacroix-river_v20211223.nc')\n",
    "    river = ff.variables['fgco2'][:] # lon is 0-360, needs to be transformed for plotting\n",
    "    ff.close()\n",
    "    print (river.shape)\n",
    "\n",
    "subregions = ('STSS-Atl','STSS-Ind','STSS-Pac',\\\n",
    "              'SPSS-Atl','SPSS-Ind','SPSS-Pac',\\\n",
    "              'ICE-Atl','ICE-Ind','ICE-Pac','STSS','SPSS','ICE','all')\n",
    "\n",
    "# 13 regions: 9 regions in SO, + 3 for combined sectors, +1 for whole SO\n",
    "\n",
    "if river_adjustment: \n",
    "    #------\n",
    "    # models\n",
    "    #------\n",
    "    river_subareas   = np.nan*np.ones([len(subregions)]) \n",
    "    biome_area_river      = np.nan*np.ones([len(subregions)]) \n",
    "    print ('Process river file ')\n",
    "    for i in range(0,len(subregions)):\n",
    "        data1 = np.copy(river)\n",
    "        reg2  = regions.ravel()  # SO RECCAP mask\n",
    "        area2 = area.ravel() # surface area\n",
    "        data2 = data1.ravel()\n",
    "            \n",
    "        if subregions[i] in ['STSS-Atl']:\n",
    "            ind_region = np.where(reg2==0)[0]\n",
    "        elif subregions[i] in ['STSS-Ind']:\n",
    "            ind_region = np.where(reg2==3)[0]\n",
    "        elif subregions[i] in ['STSS-Pac']:\n",
    "            ind_region = np.where(reg2==6)[0]\n",
    "        elif subregions[i] in ['SPSS-Atl']:\n",
    "            ind_region = np.where(reg2==1)[0]\n",
    "        elif subregions[i] in ['SPSS-Ind']:\n",
    "            ind_region = np.where(reg2==4)[0]\n",
    "        elif subregions[i] in ['SPSS-Pac']:\n",
    "            ind_region = np.where(reg2==7)[0]\n",
    "        elif subregions[i] in ['ICE-Atl']:\n",
    "            ind_region = np.where(reg2==2)[0]\n",
    "        elif subregions[i] in ['ICE-Ind']:\n",
    "            ind_region = np.where(reg2==5)[0]\n",
    "        elif subregions[i] in ['ICE-Pac']:\n",
    "            ind_region = np.where(reg2==8)[0]\n",
    "        elif subregions[i] in ['STSS']:\n",
    "            ind_region = np.where((reg2==0) | (reg2==3) | (reg2==6))[0]\n",
    "        elif subregions[i] in ['SPSS']:\n",
    "            ind_region = np.where((reg2==1) | (reg2==4) | (reg2==7))[0]\n",
    "        elif subregions[i] in ['ICE']:\n",
    "            ind_region =np.where((reg2==2) | (reg2==5) | (reg2==8))[0]\n",
    "        elif subregions[i] in ['all']:\n",
    "            ind_region = np.where(reg2>=0)[0]\n",
    "        ind_not_nan_data = np.where(~np.isnan(data2[ind_region]))[0] # only consider data points that are filled\n",
    "        total_area = np.sum(area2[ind_region][ind_not_nan_data])\n",
    "        if subregions[i] in ['all']:\n",
    "            print (total_area)\n",
    "        biome_area_river[i] = total_area \n",
    "        #total_area = np.sum(area2[ind_region])\n",
    "        if np.nansum(data1)!=0: # only for years which are filled\n",
    "            river_subareas[i] = np.nansum(data2[ind_region]*area2[ind_region]/total_area)\n",
    "        del data1,reg2,area2,data2,ind_region,total_area\n",
    "        \n",
    "#-------\n",
    "# save field to apply to model output\n",
    "#-------\n",
    "\n",
    "if river_adjustment:\n",
    "    add_to_models = np.copy(river_subareas)\n",
    "else:\n",
    "    add_to_models = np.zeros_like(river_subareas) # add a field of zeros\n",
    "print ('River fluxes to add for subregion all:',add_to_models[-1])\n",
    "\n",
    "    \n",
    "#----\n",
    "# check for river fluxes\n",
    "#----\n",
    "if river_adjustment:\n",
    "    print ('')\n",
    "    print ('River flux adjustment (pos=outgassing):')\n",
    "    factor_river = 365.25*86400.*12.011/1e15 # don't use \"-1\" here; global number should positive = outgassing due to rivers\n",
    "    for ss in range(0,len(subregions)):\n",
    "        print (subregions[ss]+': '+str(factor_river*river_subareas[ss]*biome_area_river[ss])+' Pg C yr-1')\n",
    "    # check: add up 3 subregions\n",
    "    print ('based on STSS/SPSS/ICE: '+str(np.sum(factor_river*river_subareas[9:12]*biome_area_river[9:12]))+' Pg C yr-1')\n",
    "\n",
    "    print ('Global ocean area: '+str(np.sum(area_global))+' m2')\n",
    "    print ('Global river flux: '+str(np.nansum(factor_river*np.multiply(river,area_global)))+' Pg C yr-1')\n",
    "    print ('SO/Global: '+str(100*factor_river*river_subareas[-1]*biome_area_river[-1]/np.nansum(factor_river*np.multiply(river,area_global)))+'%')\n",
    "    print ('')\n",
    "    #print river\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----\n",
    "# get trends\n",
    "#-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----\n",
    "# FUNCTIONS to get drift-corrected trends\n",
    "#----\n",
    "\n",
    "def get_trend_MMM_MDM_w_simB(biome_area_models,biome_area_data_products,models_all,data_all,\\\n",
    "                             add_to_models,biome_area_river,models,data_prod,\\\n",
    "                                               eval_time,factor,unit,slope,year_start,year_end):\n",
    "    # slope -> trend in simB over chosen year_start and year_end\n",
    "    \n",
    "    factor_river = 365.25*86400.*12.011/1e15 # for river fluxes, no need to multiply by \"-1\" -> pos=outgassing already!\n",
    "\n",
    "    #-----\n",
    "    # MODELS\n",
    "    #-----\n",
    "    add_rivers = factor_river*biome_area_river*add_to_models\n",
    "    add_each_year = add_rivers*np.ones(len(eval_time))\n",
    "    \n",
    "    # create array of area: models x time (area constant in time)\n",
    "    area_2d = np.transpose(np.tile(biome_area_models[:],[len(eval_time),1])) # should be models x time\n",
    "    \n",
    "    # apply model trends\n",
    "    models_all2 = np.zeros_like(models_all)\n",
    "    # \"slope\" is already in PgC yr-1 yr-1, \n",
    "    # make sure that when applying it to fluxes, the latter are already in Pg C yr-1\n",
    "    for mm in range(0,len(models)):\n",
    "        for yy in range(0,len(eval_time)):\n",
    "                models_all2[mm,yy] = area_2d[mm,yy]*factor*models_all[mm,yy]-slope[mm]\n",
    "                \n",
    "    # correcting with trend from year_start to year_end\n",
    "    models_all2a = np.zeros_like(models_all)\n",
    "    # \"slope\" is already in PgC yr-1 yr-1, \n",
    "    # make sure that when applying it to fluxes, the latter are already in Pg C yr-1\n",
    "    for mm in range(0,len(models)):\n",
    "        for yy in range(0,len(eval_time)):\n",
    "                models_all2a[mm,yy] = area_2d[mm,yy]*factor*models_all[mm,yy]-slope[mm]\n",
    "                \n",
    "    # for calculation of multi-model mean, use the array that is drift-corrected\n",
    "    model_data_mean = np.nanmean(models_all2[:,:],axis=0)+add_each_year\n",
    "    \n",
    "    #-----\n",
    "    # DATA -> area not constant in time, passed as an input argument \n",
    "    #-----\n",
    "    obs_data_mean  = np.nanmean(biome_area_data_products[:,:]*factor*data_all[:,:],axis=0)\n",
    "      \n",
    "    #---\n",
    "    # plot linear fits (1985-2000)\n",
    "    #---\n",
    "    \n",
    "    #year_start,year_end = 1985,2000\n",
    "    ind2000 = np.where((eval_time>=year_start) & (eval_time<=year_end))[0]\n",
    "    x = np.array(np.arange(0,len(eval_time[ind2000]))).reshape((-1, 1))\n",
    "    \n",
    "    # MODELS\n",
    "    a1 = model_data_mean[ind2000]\n",
    "    ind_noNaN = np.where(~np.isnan(a1))[0]\n",
    "    a1 = a1[ind_noNaN]\n",
    "    x1 = x[ind_noNaN]\n",
    "    model = LinearRegression()\n",
    "    model.fit(x1, a1)\n",
    "    model = LinearRegression().fit(x, a1)\n",
    "    slope_models1 = model.coef_[0]\n",
    "    # Make predictions using the testing set\n",
    "    pred_data = model.predict(np.array(np.arange(0,len(eval_time[ind2000]))).reshape((-1, 1)))\n",
    "    \n",
    "    # DATA PRODUCTS\n",
    "    a1 = obs_data_mean[ind2000]\n",
    "    ind_noNaN = np.where(~np.isnan(a1))[0]\n",
    "    a1 = a1[ind_noNaN]\n",
    "    x1 = x[ind_noNaN]\n",
    "    model = LinearRegression()\n",
    "    model.fit(x1, a1)\n",
    "    model = LinearRegression().fit(x, a1)\n",
    "    slope_data1 = model.coef_[0]\n",
    "    \n",
    "    # Make predictions using the testing set\n",
    "    pred_data = model.predict(np.array(np.arange(0,len(eval_time[ind2000]))).reshape((-1, 1)))\n",
    "    \n",
    "    # get trend in each model (to get spread)\n",
    "    # if end year is 2018, don't use CCSM (ends in 2017)\n",
    "    trend_models1 = np.nan*np.ones(len(models))\n",
    "    for mm in range(0,len(models)):\n",
    "        if year_end==2018:\n",
    "            if not models[mm] in ['CCSM-WHOI']:\n",
    "                a1 = models_all2a[mm,ind2000] # updated here: use models corrected with trend in simB from 1985-2000\n",
    "                ind_noNaN = np.where(~np.isnan(a1))[0]\n",
    "                a1 = a1[ind_noNaN]\n",
    "                x1 = x[ind_noNaN]\n",
    "                model = LinearRegression()\n",
    "                model.fit(x1, a1)\n",
    "                model = LinearRegression().fit(x, a1)\n",
    "                #print 'slope in '+models[mm]+' (per decade): '+str(10*model.coef_[0])\n",
    "                trend_models1[mm] = model.coef_[0] # store in array\n",
    "                del a1,ind_noNaN,x1,model\n",
    "        else:\n",
    "            a1 = models_all2a[mm,ind2000] # updated here: use models corrected with trend in simB from 1985-2000\n",
    "            ind_noNaN = np.where(~np.isnan(a1))[0]\n",
    "            a1 = a1[ind_noNaN]\n",
    "            x1 = x[ind_noNaN]\n",
    "            model = LinearRegression()\n",
    "            model.fit(x1, a1)\n",
    "            model = LinearRegression().fit(x, a1)\n",
    "            #print 'slope in '+models[mm]+' (per decade): '+str(10*model.coef_[0])\n",
    "            trend_models1[mm] = model.coef_[0] # store in array\n",
    "            del a1,ind_noNaN,x1,model\n",
    "    \n",
    "    # get trend in each data product (to get spread)\n",
    "    trend_data1 = np.nan*np.ones(len(data_prod))\n",
    "    for mm in range(1,len(data_prod)): # exclude AOML\n",
    "        a1 = biome_area_data_products[mm,ind2000]*factor*data_all[mm,ind2000]\n",
    "        ind_noNaN = np.where(~np.isnan(a1))[0]\n",
    "        a1 = a1[ind_noNaN]\n",
    "        x1 = x[ind_noNaN]\n",
    "        model = LinearRegression()\n",
    "        model.fit(x1, a1)\n",
    "        model = LinearRegression().fit(x, a1)\n",
    "        #print 'slope in '+data_prod[mm]+' (per decade): '+str(10*model.coef_[0])\n",
    "        trend_data1[mm] = model.coef_[0] # store in array\n",
    "        del a1,ind_noNaN,x1,model \n",
    "    \n",
    "    round2 = 1000\n",
    "    \n",
    "    #print('Linear trends in Pg C yr-1 dec-1: ')\n",
    "    print('GOBMs, '+str(year_start)+'-'+str(year_end)+': '+\\\n",
    "                    str(np.round(round2*10*np.nanmean(trend_models1))/round2)+' +/- '+\\\n",
    "                    str(np.round(round2*10*np.nanstd(trend_models1))/round2))\n",
    "    print('pCO2 products, '+str(year_start)+'-'+str(year_end)+': '+\\\n",
    "                    str(np.round(round2*10*slope_data1)/round2)+' +/- '+\\\n",
    "                    str(np.round(round2*10*np.nanstd(trend_data1))/round2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear trends in Pg C yr-1 dec-1: \n",
      "\n",
      "all, 1985-2000\n",
      "GOBMs, 1985-2000: -0.085 +/- 0.068\n",
      "pCO2 products, 1985-2000: 0.005 +/- 0.124\n",
      "GOBMs, 1986-2000: -0.076 +/- 0.066\n",
      "pCO2 products, 1986-2000: 0.025 +/- 0.13\n",
      "GOBMs, 1987-2000: -0.072 +/- 0.062\n",
      "pCO2 products, 1987-2000: 0.047 +/- 0.139\n",
      "GOBMs, 1985-1999: -0.08 +/- 0.069\n",
      "pCO2 products, 1985-1999: -0.004 +/- 0.123\n",
      "GOBMs, 1985-2001: -0.082 +/- 0.063\n",
      "pCO2 products, 1985-2001: 0.023 +/- 0.125\n",
      "GOBMs, 1985-2002: -0.1 +/- 0.066\n",
      "pCO2 products, 1985-2002: 0.016 +/- 0.124\n",
      "\n",
      "#########\n",
      "\n",
      "\n",
      "ICE, 1985-2000\n",
      "GOBMs, 1985-2000: -0.029 +/- 0.023\n",
      "pCO2 products, 1985-2000: -0.025 +/- 0.042\n",
      "GOBMs, 1986-2000: -0.029 +/- 0.02\n",
      "pCO2 products, 1986-2000: -0.025 +/- 0.044\n",
      "GOBMs, 1987-2000: -0.024 +/- 0.017\n",
      "pCO2 products, 1987-2000: -0.019 +/- 0.045\n",
      "GOBMs, 1985-1999: -0.03 +/- 0.026\n",
      "pCO2 products, 1985-1999: -0.024 +/- 0.038\n",
      "GOBMs, 1985-2001: -0.029 +/- 0.022\n",
      "pCO2 products, 1985-2001: -0.019 +/- 0.04\n",
      "GOBMs, 1985-2002: -0.03 +/- 0.02\n",
      "pCO2 products, 1985-2002: -0.022 +/- 0.041\n",
      "\n",
      "#########\n",
      "\n",
      "\n",
      "SPSS, 1985-2000\n",
      "GOBMs, 1985-2000: -0.034 +/- 0.044\n",
      "pCO2 products, 1985-2000: 0.026 +/- 0.066\n",
      "GOBMs, 1986-2000: -0.028 +/- 0.049\n",
      "pCO2 products, 1986-2000: 0.035 +/- 0.069\n",
      "GOBMs, 1987-2000: -0.023 +/- 0.049\n",
      "pCO2 products, 1987-2000: 0.045 +/- 0.074\n",
      "GOBMs, 1985-1999: -0.035 +/- 0.045\n",
      "pCO2 products, 1985-1999: 0.018 +/- 0.067\n",
      "GOBMs, 1985-2001: -0.034 +/- 0.042\n",
      "pCO2 products, 1985-2001: 0.035 +/- 0.065\n",
      "GOBMs, 1985-2002: -0.043 +/- 0.041\n",
      "pCO2 products, 1985-2002: 0.033 +/- 0.065\n",
      "\n",
      "#########\n",
      "\n",
      "\n",
      "STSS, 1985-2000\n",
      "GOBMs, 1985-2000: -0.022 +/- 0.02\n",
      "pCO2 products, 1985-2000: 0.004 +/- 0.032\n",
      "GOBMs, 1986-2000: -0.019 +/- 0.018\n",
      "pCO2 products, 1986-2000: 0.015 +/- 0.032\n",
      "GOBMs, 1987-2000: -0.025 +/- 0.019\n",
      "pCO2 products, 1987-2000: 0.022 +/- 0.033\n",
      "GOBMs, 1985-1999: -0.015 +/- 0.02\n",
      "pCO2 products, 1985-1999: 0.001 +/- 0.034\n",
      "GOBMs, 1985-2001: -0.019 +/- 0.019\n",
      "pCO2 products, 1985-2001: 0.008 +/- 0.033\n",
      "GOBMs, 1985-2002: -0.027 +/- 0.02\n",
      "pCO2 products, 1985-2002: 0.005 +/- 0.032\n"
     ]
    }
   ],
   "source": [
    "#------\n",
    "# get trends \n",
    "#-------\n",
    "\n",
    "# CHECK: the list below should match the one at the very top of this script\n",
    "models_short = ('CCSM_WHOI','CESM_ETHZ','CNRM_ESM2_1','EC_Earth3','FESOM_REcoM_HR','FESOM_REcoM_LR',\\\n",
    "             'MOM6_Princeton','MPIOM_HAMOCC','MRI_ESM2_1','NorESM_OC1.2',\\\n",
    "             'ORCA025_GEOMAR','ORCA1_LIM3_PISCES','PlankTOM12','ROMS-SouthernOcean-ETHZ')\n",
    "\n",
    "# trend is in Pg C yr-1 dec-1\n",
    "\n",
    "# -> divide by 10 to get Pg C yr-1 yr-1\n",
    "# -> subtract this from flux of each year in sim A -> DOUBLE-CHECK THE SIGN!!!!\n",
    "\n",
    "unit    = 'FCO$_{2}$ (Pg C yr$^{-1}$)'\n",
    "factor2 = -1*365.25*86400.*12.011/1e15 # conversion factor from mol C m-2 s-1 to PgC yr-1\n",
    "\n",
    "# transpose arrays for function:\n",
    "# expected input: data: models x regions x time, data_mean: regions x time; swap dimensions in arrays\n",
    "flux_models_subareas_plot     = np.transpose(flux_models_subareas,[1,2,0])\n",
    "flux_data_products_plot       = np.transpose(flux_data_products_subareas,[1,2,0])\n",
    "flux_data_assim_subareas_plot = np.transpose(flux_data_assim_subareas,[1,2,0])\n",
    "flux_data_ocim_subareas_plot  = np.transpose(flux_ocim_subareas,[1,2,0])\n",
    "flux_atminv_subareas_plot  = np.transpose(flux_atminv_subareas,[1,2,0])\n",
    "#multi_model_mean_plot = np.mean(flux_models_subareas_plot,axis=0)\n",
    "multi_model_mean_plot      = np.transpose(multi_model_mean,[1,0])\n",
    "multi_data_prod_mean_plot  = np.transpose(multi_data_prod_mean,[1,0])\n",
    "multi_data_assim_mean_plot = np.transpose(multi_data_assim_mean,[1,0])\n",
    "multi_data_watson_mean_plot = np.transpose(multi_data_watson_mean,[1,0])\n",
    "multi_atminv_mean_plot = np.transpose(multi_atminv_mean,[1,0])\n",
    "multi_soccom_mean_plot = np.transpose(multi_soccom_mean,[1,0])\n",
    "multi_soccom_mean_plot[:,0:-4] = np.nan # only show years 2015-2018 for SOCCOM products\n",
    "\n",
    "\n",
    "#----\n",
    "# get trends for chosen year_start and year_end\n",
    "#----\n",
    "\n",
    "year_start,year_end = 1985,2000\n",
    "\n",
    "# which years do I want?\n",
    "# 1985-2000 IN PAPER\n",
    "# 1986-2000\n",
    "# 1987-2000\n",
    "\n",
    "# 1985-1999\n",
    "# 1985-2001\n",
    "# 1985-2002\n",
    "\n",
    "# 2001-2018 IN PAPER\n",
    "# 1999-2018\n",
    "# 2002-2018\n",
    "# 2003-2018\n",
    "\n",
    "# 2001-2017\n",
    "# 2001-2016\n",
    "\n",
    "year_start_list = [1985,1986,1987,1985,1985,1985]\n",
    "year_end_list   = [2000,2000,2000,1999,2001,2002]\n",
    "print('Linear trends in Pg C yr-1 dec-1: ')\n",
    "    \n",
    "for yy in range(0,len(year_start_list)):\n",
    "    year_start,year_end = year_start_list[yy],year_end_list[yy]\n",
    "    #----\n",
    "    # load trends for simB\n",
    "    #----\n",
    "    f1 = Dataset(path_trend+'LinearTrend_'+str(year_start)+'_'+str(year_end)+'_CO2_flux_simB.nc')\n",
    "    slope_models_simB = np.zeros([len(models),len(subregions)])\n",
    "    for mm in range(0,len(models)):\n",
    "        slope_models_simB[mm,:] = f1.variables['trend_fgco2_'+models_short[mm]][:]/10\n",
    "    f1.close()\n",
    "    \n",
    "    rr = subregions.index(\"all\") # region index\n",
    "    if yy==0:\n",
    "        print ('')   \n",
    "        print (subregions[rr]+', '+str(year_start)+'-'+str(year_end))\n",
    "    get_trend_MMM_MDM_w_simB(biome_area_models[:,rr],np.transpose(biome_area_data_products_varying[:,:,rr]),\\\n",
    "                                                    flux_models_subareas_plot[:,rr,:],\\\n",
    "                                                    flux_data_products_plot[:,rr,:],\\\n",
    "                                                    add_to_models[rr],biome_area_river[rr],\\\n",
    "                                                    models,data_prod,eval_time,factor2,unit,slope_models_simB[:,rr],year_start,year_end)\n",
    "print()\n",
    "print('#########')\n",
    "print() \n",
    "#---\n",
    "# ICE\n",
    "#---\n",
    "for yy in range(0,len(year_start_list)):\n",
    "    year_start,year_end = year_start_list[yy],year_end_list[yy]\n",
    "    #----\n",
    "    # load trends for simB\n",
    "    #----\n",
    "    f1 = Dataset(path_trend+'LinearTrend_'+str(year_start)+'_'+str(year_end)+'_CO2_flux_simB.nc')\n",
    "    slope_models_simB = np.zeros([len(models),len(subregions)])\n",
    "    for mm in range(0,len(models)):\n",
    "        slope_models_simB[mm,:] = f1.variables['trend_fgco2_'+models_short[mm]][:]/10\n",
    "    f1.close()\n",
    "    \n",
    "    rr = subregions.index(\"ICE\") # region index\n",
    "    if yy==0:\n",
    "        print ('')   \n",
    "        print (subregions[rr]+', '+str(year_start)+'-'+str(year_end))\n",
    "    get_trend_MMM_MDM_w_simB(biome_area_models[:,rr],np.transpose(biome_area_data_products_varying[:,:,rr]),\\\n",
    "                                                    flux_models_subareas_plot[:,rr,:],\\\n",
    "                                                    flux_data_products_plot[:,rr,:],\\\n",
    "                                                    add_to_models[rr],biome_area_river[rr],\\\n",
    "                                                    models,data_prod,eval_time,factor2,unit,slope_models_simB[:,rr],year_start,year_end)\n",
    "print()\n",
    "print('#########')\n",
    "print()\n",
    "#---\n",
    "# SPSS\n",
    "#---\n",
    "for yy in range(0,len(year_start_list)):\n",
    "    year_start,year_end = year_start_list[yy],year_end_list[yy]\n",
    "    #----\n",
    "    # load trends for simB\n",
    "    #----\n",
    "    f1 = Dataset(path_trend+'LinearTrend_'+str(year_start)+'_'+str(year_end)+'_CO2_flux_simB.nc')\n",
    "    slope_models_simB = np.zeros([len(models),len(subregions)])\n",
    "    for mm in range(0,len(models)):\n",
    "        slope_models_simB[mm,:] = f1.variables['trend_fgco2_'+models_short[mm]][:]/10\n",
    "    f1.close()\n",
    "    \n",
    "    rr = subregions.index(\"SPSS\") # region index\n",
    "    if yy==0:\n",
    "        print ('')   \n",
    "        print (subregions[rr]+', '+str(year_start)+'-'+str(year_end))\n",
    "    get_trend_MMM_MDM_w_simB(biome_area_models[:,rr],np.transpose(biome_area_data_products_varying[:,:,rr]),\\\n",
    "                                                    flux_models_subareas_plot[:,rr,:],\\\n",
    "                                                    flux_data_products_plot[:,rr,:],\\\n",
    "                                                    add_to_models[rr],biome_area_river[rr],\\\n",
    "                                                    models,data_prod,eval_time,factor2,unit,slope_models_simB[:,rr],year_start,year_end)\n",
    "    \n",
    "print()\n",
    "print('#########')\n",
    "print()\n",
    "#---\n",
    "# STSS\n",
    "#---\n",
    "for yy in range(0,len(year_start_list)):\n",
    "    year_start,year_end = year_start_list[yy],year_end_list[yy]\n",
    "    #----\n",
    "    # load trends for simB\n",
    "    #----\n",
    "    f1 = Dataset(path_trend+'LinearTrend_'+str(year_start)+'_'+str(year_end)+'_CO2_flux_simB.nc')\n",
    "    slope_models_simB = np.zeros([len(models),len(subregions)])\n",
    "    for mm in range(0,len(models)):\n",
    "        slope_models_simB[mm,:] = f1.variables['trend_fgco2_'+models_short[mm]][:]/10\n",
    "    f1.close()\n",
    "    \n",
    "    rr = subregions.index(\"STSS\") # region index\n",
    "    if yy==0:\n",
    "        print ('')   \n",
    "        print (subregions[rr]+', '+str(year_start)+'-'+str(year_end))\n",
    "    get_trend_MMM_MDM_w_simB(biome_area_models[:,rr],np.transpose(biome_area_data_products_varying[:,:,rr]),\\\n",
    "                                                    flux_models_subareas_plot[:,rr,:],\\\n",
    "                                                    flux_data_products_plot[:,rr,:],\\\n",
    "                                                    add_to_models[rr],biome_area_river[rr],\\\n",
    "                                                    models,data_prod,eval_time,factor2,unit,slope_models_simB[:,rr],year_start,year_end)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear trends in Pg C yr-1 dec-1: \n",
      "\n",
      "all, 2001-2018\n",
      "GOBMs, 2001-2018: -0.113 +/- 0.028\n",
      "pCO2 products, 2001-2018: -0.258 +/- 0.059\n",
      "GOBMs, 1999-2018: -0.127 +/- 0.03\n",
      "pCO2 products, 1999-2018: -0.255 +/- 0.062\n",
      "GOBMs, 2000-2018: -0.118 +/- 0.03\n",
      "pCO2 products, 2000-2018: -0.257 +/- 0.06\n",
      "GOBMs, 2002-2018: -0.097 +/- 0.033\n",
      "pCO2 products, 2002-2018: -0.241 +/- 0.058\n",
      "GOBMs, 2003-2018: -0.105 +/- 0.034\n",
      "pCO2 products, 2003-2018: -0.239 +/- 0.048\n",
      "GOBMs, 2001-2017: -0.111 +/- 0.03\n",
      "pCO2 products, 2001-2017: -0.262 +/- 0.072\n",
      "GOBMs, 2001-2016: -0.108 +/- 0.03\n",
      "pCO2 products, 2001-2016: -0.27 +/- 0.082\n",
      "\n",
      "#########\n",
      "\n",
      "\n",
      "ICE, 2001-2018\n",
      "GOBMs, 2001-2018: -0.035 +/- 0.015\n",
      "pCO2 products, 2001-2018: -0.064 +/- 0.038\n",
      "GOBMs, 1999-2018: -0.036 +/- 0.015\n",
      "pCO2 products, 1999-2018: -0.065 +/- 0.039\n",
      "GOBMs, 2000-2018: -0.036 +/- 0.015\n",
      "pCO2 products, 2000-2018: -0.063 +/- 0.037\n",
      "GOBMs, 2002-2018: -0.033 +/- 0.016\n",
      "pCO2 products, 2002-2018: -0.056 +/- 0.035\n",
      "GOBMs, 2003-2018: -0.032 +/- 0.016\n",
      "pCO2 products, 2003-2018: -0.054 +/- 0.033\n",
      "GOBMs, 2001-2017: -0.031 +/- 0.016\n",
      "pCO2 products, 2001-2017: -0.066 +/- 0.041\n",
      "GOBMs, 2001-2016: -0.029 +/- 0.016\n",
      "pCO2 products, 2001-2016: -0.07 +/- 0.046\n",
      "\n",
      "#########\n",
      "\n",
      "\n",
      "SPSS, 2001-2018\n",
      "GOBMs, 2001-2018: -0.043 +/- 0.021\n",
      "pCO2 products, 2001-2018: -0.089 +/- 0.015\n",
      "GOBMs, 1999-2018: -0.053 +/- 0.019\n",
      "pCO2 products, 1999-2018: -0.088 +/- 0.02\n",
      "GOBMs, 2000-2018: -0.048 +/- 0.021\n",
      "pCO2 products, 2000-2018: -0.09 +/- 0.017\n",
      "GOBMs, 2002-2018: -0.036 +/- 0.025\n",
      "pCO2 products, 2002-2018: -0.083 +/- 0.016\n",
      "GOBMs, 2003-2018: -0.039 +/- 0.022\n",
      "pCO2 products, 2003-2018: -0.081 +/- 0.015\n",
      "GOBMs, 2001-2017: -0.041 +/- 0.024\n",
      "pCO2 products, 2001-2017: -0.089 +/- 0.022\n",
      "GOBMs, 2001-2016: -0.039 +/- 0.024\n",
      "pCO2 products, 2001-2016: -0.091 +/- 0.028\n",
      "\n",
      "#########\n",
      "\n",
      "\n",
      "STSS, 2001-2018\n",
      "GOBMs, 2001-2018: -0.035 +/- 0.01\n",
      "pCO2 products, 2001-2018: -0.105 +/- 0.03\n",
      "GOBMs, 1999-2018: -0.038 +/- 0.009\n",
      "pCO2 products, 1999-2018: -0.101 +/- 0.029\n",
      "GOBMs, 2000-2018: -0.034 +/- 0.009\n",
      "pCO2 products, 2000-2018: -0.103 +/- 0.03\n",
      "GOBMs, 2002-2018: -0.028 +/- 0.011\n",
      "pCO2 products, 2002-2018: -0.102 +/- 0.03\n",
      "GOBMs, 2003-2018: -0.033 +/- 0.008\n",
      "pCO2 products, 2003-2018: -0.104 +/- 0.029\n",
      "GOBMs, 2001-2017: -0.038 +/- 0.011\n",
      "pCO2 products, 2001-2017: -0.107 +/- 0.034\n",
      "GOBMs, 2001-2016: -0.039 +/- 0.011\n",
      "pCO2 products, 2001-2016: -0.109 +/- 0.034\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2001-2018 IN PAPER\n",
    "# 1999-2018\n",
    "# 2002-2018\n",
    "# 2003-2018\n",
    "\n",
    "# 2001-2017\n",
    "# 2001-2016\n",
    "\n",
    "year_start_list = [2001,1999,2000,2002,2003,2001,2001]\n",
    "year_end_list   = [2018,2018,2018,2018,2018,2017,2016]\n",
    "print('Linear trends in Pg C yr-1 dec-1: ')\n",
    "    \n",
    "for yy in range(0,len(year_start_list)):\n",
    "    year_start,year_end = year_start_list[yy],year_end_list[yy]\n",
    "    #----\n",
    "    # load trends for simB\n",
    "    #----\n",
    "    f1 = Dataset(path_trend+'LinearTrend_'+str(year_start)+'_'+str(year_end)+'_CO2_flux_simB.nc')\n",
    "    slope_models_simB = np.zeros([len(models),len(subregions)])\n",
    "    for mm in range(0,len(models)):\n",
    "        slope_models_simB[mm,:] = f1.variables['trend_fgco2_'+models_short[mm]][:]/10\n",
    "    f1.close()\n",
    "\n",
    "    rr = subregions.index(\"all\") # region index\n",
    "    if yy==0:\n",
    "        print ('')   \n",
    "        print (subregions[rr]+', '+str(year_start)+'-'+str(year_end))\n",
    "    get_trend_MMM_MDM_w_simB(biome_area_models[:,rr],np.transpose(biome_area_data_products_varying[:,:,rr]),\\\n",
    "                                                    flux_models_subareas_plot[:,rr,:],\\\n",
    "                                                    flux_data_products_plot[:,rr,:],\\\n",
    "                                                    add_to_models[rr],biome_area_river[rr],\\\n",
    "                                                    models,data_prod,eval_time,factor2,unit,slope_models_simB[:,rr],year_start,year_end)\n",
    "    \n",
    "print()\n",
    "print('#########')\n",
    "print() \n",
    "#---\n",
    "# ICE\n",
    "#---\n",
    "for yy in range(0,len(year_start_list)):\n",
    "    year_start,year_end = year_start_list[yy],year_end_list[yy]\n",
    "    #----\n",
    "    # load trends for simB\n",
    "    #----\n",
    "    f1 = Dataset(path_trend+'LinearTrend_'+str(year_start)+'_'+str(year_end)+'_CO2_flux_simB.nc')\n",
    "    slope_models_simB = np.zeros([len(models),len(subregions)])\n",
    "    for mm in range(0,len(models)):\n",
    "        slope_models_simB[mm,:] = f1.variables['trend_fgco2_'+models_short[mm]][:]/10\n",
    "    f1.close()\n",
    "\n",
    "    rr = subregions.index(\"ICE\") # region index\n",
    "    if yy==0:\n",
    "        print ('')   \n",
    "        print (subregions[rr]+', '+str(year_start)+'-'+str(year_end))\n",
    "    get_trend_MMM_MDM_w_simB(biome_area_models[:,rr],np.transpose(biome_area_data_products_varying[:,:,rr]),\\\n",
    "                                                    flux_models_subareas_plot[:,rr,:],\\\n",
    "                                                    flux_data_products_plot[:,rr,:],\\\n",
    "                                                    add_to_models[rr],biome_area_river[rr],\\\n",
    "                                                    models,data_prod,eval_time,factor2,unit,slope_models_simB[:,rr],year_start,year_end)\n",
    "print()\n",
    "print('#########')\n",
    "print()\n",
    "#---\n",
    "# SPSS\n",
    "#---\n",
    "for yy in range(0,len(year_start_list)):\n",
    "    year_start,year_end = year_start_list[yy],year_end_list[yy]\n",
    "    #----\n",
    "    # load trends for simB\n",
    "    #----\n",
    "    f1 = Dataset(path_trend+'LinearTrend_'+str(year_start)+'_'+str(year_end)+'_CO2_flux_simB.nc')\n",
    "    slope_models_simB = np.zeros([len(models),len(subregions)])\n",
    "    for mm in range(0,len(models)):\n",
    "        slope_models_simB[mm,:] = f1.variables['trend_fgco2_'+models_short[mm]][:]/10\n",
    "    f1.close()\n",
    "\n",
    "    rr = subregions.index(\"SPSS\") # region index\n",
    "    if yy==0:\n",
    "        print ('')   \n",
    "        print (subregions[rr]+', '+str(year_start)+'-'+str(year_end))\n",
    "    get_trend_MMM_MDM_w_simB(biome_area_models[:,rr],np.transpose(biome_area_data_products_varying[:,:,rr]),\\\n",
    "                                                    flux_models_subareas_plot[:,rr,:],\\\n",
    "                                                    flux_data_products_plot[:,rr,:],\\\n",
    "                                                    add_to_models[rr],biome_area_river[rr],\\\n",
    "                                                    models,data_prod,eval_time,factor2,unit,slope_models_simB[:,rr],year_start,year_end)\n",
    "    \n",
    "print()\n",
    "print('#########')\n",
    "print()\n",
    "#---\n",
    "# STSS\n",
    "#---\n",
    "for yy in range(0,len(year_start_list)):\n",
    "    year_start,year_end = year_start_list[yy],year_end_list[yy]\n",
    "    #----\n",
    "    # load trends for simB\n",
    "    #----\n",
    "    f1 = Dataset(path_trend+'LinearTrend_'+str(year_start)+'_'+str(year_end)+'_CO2_flux_simB.nc')\n",
    "    slope_models_simB = np.zeros([len(models),len(subregions)])\n",
    "    for mm in range(0,len(models)):\n",
    "        slope_models_simB[mm,:] = f1.variables['trend_fgco2_'+models_short[mm]][:]/10\n",
    "    f1.close()\n",
    "\n",
    "    rr = subregions.index(\"STSS\") # region index\n",
    "    if yy==0:\n",
    "        print ('')   \n",
    "        print (subregions[rr]+', '+str(year_start)+'-'+str(year_end))\n",
    "    get_trend_MMM_MDM_w_simB(biome_area_models[:,rr],np.transpose(biome_area_data_products_varying[:,:,rr]),\\\n",
    "                                                    flux_models_subareas_plot[:,rr,:],\\\n",
    "                                                    flux_data_products_plot[:,rr,:],\\\n",
    "                                                    add_to_models[rr],biome_area_river[rr],\\\n",
    "                                                    models,data_prod,eval_time,factor2,unit,slope_models_simB[:,rr],year_start,year_end)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv-jupyter",
   "language": "python",
   "name": "myenv-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
